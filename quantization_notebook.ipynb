{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd0a2c7e-76d0-41d5-a8fb-17a72e1643ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edc941b1-8ad8-4342-8781-9d4c2bf6b924",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df6b807de2684cc6b7a49edea6f8323e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22258286-7ba4-4499-bec2-9cd3e8370352",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download , hf_hub_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3d4ec95-d155-4f1c-b66f-63e572110d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/huggingface_hub/file_download.py:1212: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\n",
      "For more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "229d542785134357b3d1a25c36f2e34d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0244230dd16487cbd317efe0bd18c43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b6aaa4e3f5b47c0b79a8d26baad4596",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes:   0%|          | 0.00/1.52k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1121828a429341ebaedfb344cd9d77fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/752 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e11b3f33be984f9f8a0e0dd687aee737",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/166M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'/teamspace/studios/this_studio/peft_model_final'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_id = \"awais009/sql_gpt_latest\"\n",
    "quantized_path = \"./quantized_models_final/\"\n",
    "peft_model_dir = \"./peft_model_final\"\n",
    "snapshot_download(repo_id = model_id, \n",
    "                  local_dir = peft_model_dir , \n",
    "                  local_dir_use_symlinks = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "474cca36-8821-439b-b520-2cd6b290f2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b59c29f-1e50-44b7-a8ab-a65eae550eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69e78496-5cab-4af4-8359-5f0665e7d42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_name = \"google/gemma-2-2b-it\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6f391a5-d671-4231-9250-5322b8d277e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1f6297749be49099ac2d13502549281",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name, \n",
    "    low_cpu_mem_usage = True ,\n",
    "    return_dict = True , \n",
    "    torch_dtype = torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e7df725-310e-426d-839e-a1b0b0f34877",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55aee32f31db43cb96fe388333dd4181",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/752 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53516422da1943a4841f898725e12cc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/166M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = PeftModel.from_pretrained(base_model , model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da6e4e8e-caa0-432d-8c2b-8ae9915a058a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "116cff34-4316-4178-afad-573b63d2b261",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"merged_model_final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c88cd9de-5a30-4513-beaf-b1b5e34e4c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b\", trust_remote_code = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "338d0374-3797-4b9e-bc67-123f7efd74de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('merged_model_final/tokenizer_config.json',\n",
       " 'merged_model_final/special_tokens_map.json',\n",
       " 'merged_model_final/tokenizer.model',\n",
       " 'merged_model_final/added_tokens.json',\n",
       " 'merged_model_final/tokenizer.json')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"merged_model_final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ccad02b6-9a56-4830-b8b3-515e0fc0966b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"GET /meta-llama/Llama-2-7b-hf/resolve/main/config.json HTTP/11\" 403 167\n",
      "ERROR:convert_hf_to_gguf_update:Failed to download model llama-spm. Error: 403 Client Error: Forbidden for url: https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/config.json\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"GET /meta-llama/Meta-Llama-3-8B/resolve/main/config.json HTTP/11\" 403 171\n",
      "ERROR:convert_hf_to_gguf_update:Failed to download model llama-bpe. Error: 403 Client Error: Forbidden for url: https://huggingface.co/meta-llama/Meta-Llama-3-8B/resolve/main/config.json\n",
      "INFO:convert_hf_to_gguf_update:phi-3: File models/tokenizers/phi-3/config.json already exists - skipping\n",
      "INFO:convert_hf_to_gguf_update:phi-3: File models/tokenizers/phi-3/tokenizer.json already exists - skipping\n",
      "INFO:convert_hf_to_gguf_update:phi-3: File models/tokenizers/phi-3/tokenizer_config.json already exists - skipping\n",
      "INFO:convert_hf_to_gguf_update:phi-3: File models/tokenizers/phi-3/tokenizer.model already exists - skipping\n",
      "INFO:convert_hf_to_gguf_update:deepseek-llm: File models/tokenizers/deepseek-llm/config.json already exists - skipping\n",
      "INFO:convert_hf_to_gguf_update:deepseek-llm: File models/tokenizers/deepseek-llm/tokenizer.json already exists - skipping\n",
      "INFO:convert_hf_to_gguf_update:deepseek-llm: File models/tokenizers/deepseek-llm/tokenizer_config.json already exists - skipping\n",
      "INFO:convert_hf_to_gguf_update:deepseek-coder: File models/tokenizers/deepseek-coder/config.json already exists - skipping\n",
      "INFO:convert_hf_to_gguf_update:deepseek-coder: File models/tokenizers/deepseek-coder/tokenizer.json already exists - skipping\n",
      "INFO:convert_hf_to_gguf_update:deepseek-coder: File models/tokenizers/deepseek-coder/tokenizer_config.json already exists - skipping\n",
      "INFO:convert_hf_to_gguf_update:falcon: File models/tokenizers/falcon/config.json already exists - skipping\n",
      "INFO:convert_hf_to_gguf_update:falcon: File models/tokenizers/falcon/tokenizer.json already exists - skipping\n",
      "INFO:convert_hf_to_gguf_update:falcon: File models/tokenizers/falcon/tokenizer_config.json already exists - skipping\n",
      "INFO:convert_hf_to_gguf_update:bert-bge: File models/tokenizers/bert-bge/config.json already exists - skipping\n",
      "INFO:convert_hf_to_gguf_update:bert-bge: File models/tokenizers/bert-bge/tokenizer.json already exists - skipping\n",
      "INFO:convert_hf_to_gguf_update:bert-bge: File models/tokenizers/bert-bge/tokenizer_config.json already exists - skipping\n",
      "INFO:convert_hf_to_gguf_update:mpt: File models/tokenizers/mpt/config.json already exists - skipping\n",
      "INFO:convert_hf_to_gguf_update:mpt: File models/tokenizers/mpt/tokenizer.json already exists - skipping\n",
      "INFO:convert_hf_to_gguf_update:mpt: File models/tokenizers/mpt/tokenizer_config.json already exists - skipping\n",
      "INFO:convert_hf_to_gguf_update:starcoder: File models/tokenizers/starcoder/config.json already exists - skipping\n",
      "INFO:convert_hf_to_gguf_update:starcoder: File models/tokenizers/starcoder/tokenizer.json already exists - skipping\n",
      "INFO:convert_hf_to_gguf_update:starcoder: File models/tokenizers/starcoder/tokenizer_config.json already exists - skipping\n",
      "INFO:convert_hf_to_gguf_update:gpt-2: File models/tokenizers/gpt-2/config.json already exists - skipping\n",
      "INFO:convert_hf_to_gguf_update:gpt-2: File models/tokenizers/gpt-2/tokenizer.json already exists - skipping\n",
      "INFO:convert_hf_to_gguf_update:gpt-2: File models/tokenizers/gpt-2/tokenizer_config.json already exists - skipping\n",
      "INFO:convert_hf_to_gguf_update:stablelm2: File models/tokenizers/stablelm2/config.json already exists - skipping\n",
      "INFO:convert_hf_to_gguf_update:stablelm2: File models/tokenizers/stablelm2/tokenizer.json already exists - skipping\n",
      "INFO:convert_hf_to_gguf_update:stablelm2: File models/tokenizers/stablelm2/tokenizer_config.json already exists - skipping\n",
      "INFO:convert_hf_to_gguf_update:refact: File models/tokenizers/refact/config.json already exists - skipping\n",
      "INFO:convert_hf_to_gguf_update:refact: File models/tokenizers/refact/tokenizer.json already exists - skipping\n",
      "INFO:convert_hf_to_gguf_update:refact: File models/tokenizers/refact/tokenizer_config.json already exists - skipping\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"GET /CohereForAI/c4ai-command-r-v01/resolve/main/config.json HTTP/11\" 403 179\n",
      "ERROR:convert_hf_to_gguf_update:Failed to download model command-r. Error: 403 Client Error: Forbidden for url: https://huggingface.co/CohereForAI/c4ai-command-r-v01/resolve/main/config.json\n",
      "INFO:convert_hf_to_gguf_update:qwen2: File models/tokenizers/qwen2/config.json already exists - skipping\n",
      "INFO:convert_hf_to_gguf_update:qwen2: File models/tokenizers/qwen2/tokenizer.json already exists - skipping\n",
      "INFO:convert_hf_to_gguf_update:qwen2: File models/tokenizers/qwen2/tokenizer_config.json already exists - skipping\n",
      "INFO:convert_hf_to_gguf_update:olmo: File models/tokenizers/olmo/config.json already exists - skipping\n",
      "INFO:convert_hf_to_gguf_update:olmo: File models/tokenizers/olmo/tokenizer.json already exists - skipping\n",
      "INFO:convert_hf_to_gguf_update:olmo: File models/tokenizers/olmo/tokenizer_config.json already exists - skipping\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"GET /databricks/dbrx-base/resolve/main/config.json HTTP/11\" 403 159\n",
      "ERROR:convert_hf_to_gguf_update:Failed to download model dbrx. Error: 403 Client Error: Forbidden for url: https://huggingface.co/databricks/dbrx-base/resolve/main/config.json\n",
      "INFO:convert_hf_to_gguf_update:jina-v2-en: File models/tokenizers/jina-v2-en/config.json already exists - skipping\n",
      "INFO:convert_hf_to_gguf_update:jina-v2-en: File models/tokenizers/jina-v2-en/tokenizer.json already exists - skipping\n",
      "INFO:convert_hf_to_gguf_update:jina-v2-en: File models/tokenizers/jina-v2-en/tokenizer_config.json already exists - skipping\n",
      "INFO:convert_hf_to_gguf_update:jina-v2-es: File models/tokenizers/jina-v2-es/config.json already exists - skipping\n",
      "INFO:convert_hf_to_gguf_update:jina-v2-es: File models/tokenizers/jina-v2-es/tokenizer.json already exists - skipping\n",
      "INFO:convert_hf_to_gguf_update:jina-v2-es: File models/tokenizers/jina-v2-es/tokenizer_config.json already exists - skipping\n",
      "INFO:convert_hf_to_gguf_update:jina-v2-de: File models/tokenizers/jina-v2-de/config.json already exists - skipping\n",
      "INFO:convert_hf_to_gguf_update:jina-v2-de: File models/tokenizers/jina-v2-de/tokenizer.json already exists - skipping\n",
      "INFO:convert_hf_to_gguf_update:jina-v2-de: File models/tokenizers/jina-v2-de/tokenizer_config.json already exists - skipping\n",
      "INFO:convert_hf_to_gguf_update:smaug-bpe: File models/tokenizers/smaug-bpe/config.json already exists - skipping\n",
      "INFO:convert_hf_to_gguf_update:smaug-bpe: File models/tokenizers/smaug-bpe/tokenizer.json already exists - skipping\n",
      "INFO:convert_hf_to_gguf_update:smaug-bpe: File models/tokenizers/smaug-bpe/tokenizer_config.json already exists - skipping\n",
      "INFO:convert_hf_to_gguf_update:poro-chat: File models/tokenizers/poro-chat/config.json already exists - skipping\n",
      "INFO:convert_hf_to_gguf_update:poro-chat: File models/tokenizers/poro-chat/tokenizer.json already exists - skipping\n",
      "INFO:convert_hf_to_gguf_update:poro-chat: File models/tokenizers/poro-chat/tokenizer_config.json already exists - skipping\n",
      "INFO:convert_hf_to_gguf_update:jina-v2-code: File models/tokenizers/jina-v2-code/config.json already exists - skipping\n",
      "INFO:convert_hf_to_gguf_update:jina-v2-code: File models/tokenizers/jina-v2-code/tokenizer.json already exists - skipping\n",
      "INFO:convert_hf_to_gguf_update:jina-v2-code: File models/tokenizers/jina-v2-code/tokenizer_config.json already exists - skipping\n",
      "INFO:convert_hf_to_gguf_update:viking: File models/tokenizers/viking/config.json already exists - skipping\n",
      "INFO:convert_hf_to_gguf_update:viking: File models/tokenizers/viking/tokenizer.json already exists - skipping\n",
      "INFO:convert_hf_to_gguf_update:viking: File models/tokenizers/viking/tokenizer_config.json already exists - skipping\n",
      "INFO:convert_hf_to_gguf_update:gemma: File models/tokenizers/gemma/config.json already exists - skipping\n",
      "INFO:convert_hf_to_gguf_update:gemma: File models/tokenizers/gemma/tokenizer.json already exists - skipping\n",
      "INFO:convert_hf_to_gguf_update:gemma: File models/tokenizers/gemma/tokenizer_config.json already exists - skipping\n",
      "INFO:convert_hf_to_gguf_update:gemma: File models/tokenizers/gemma/tokenizer.model already exists - skipping\n",
      "INFO:convert_hf_to_gguf_update:gemma-2: File models/tokenizers/gemma-2/config.json already exists - skipping\n",
      "INFO:convert_hf_to_gguf_update:gemma-2: File models/tokenizers/gemma-2/tokenizer.json already exists - skipping\n",
      "INFO:convert_hf_to_gguf_update:gemma-2: File models/tokenizers/gemma-2/tokenizer_config.json already exists - skipping\n",
      "INFO:convert_hf_to_gguf_update:gemma-2: File models/tokenizers/gemma-2/tokenizer.model already exists - skipping\n",
      "INFO:convert_hf_to_gguf_update:jais: File models/tokenizers/jais/config.json already exists - skipping\n",
      "INFO:convert_hf_to_gguf_update:jais: File models/tokenizers/jais/tokenizer.json already exists - skipping\n",
      "INFO:convert_hf_to_gguf_update:jais: File models/tokenizers/jais/tokenizer_config.json already exists - skipping\n",
      "INFO:convert_hf_to_gguf_update:t5: File models/tokenizers/t5/config.json already exists - skipping\n",
      "INFO:convert_hf_to_gguf_update:t5: File models/tokenizers/t5/tokenizer.json already exists - skipping\n",
      "INFO:convert_hf_to_gguf_update:t5: File models/tokenizers/t5/tokenizer_config.json already exists - skipping\n",
      "INFO:convert_hf_to_gguf_update:t5: File models/tokenizers/t5/spiece.model already exists - skipping\n",
      "INFO:convert_hf_to_gguf_update:codeshell: File models/tokenizers/codeshell/config.json already exists - skipping\n",
      "INFO:convert_hf_to_gguf_update:codeshell: File models/tokenizers/codeshell/tokenizer.json already exists - skipping\n",
      "INFO:convert_hf_to_gguf_update:codeshell: File models/tokenizers/codeshell/tokenizer_config.json already exists - skipping\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"GET /mistralai/Mistral-Nemo-Base-2407/resolve/main/config.json HTTP/11\" 403 183\n",
      "ERROR:convert_hf_to_gguf_update:Failed to download model tekken. Error: 403 Client Error: Forbidden for url: https://huggingface.co/mistralai/Mistral-Nemo-Base-2407/resolve/main/config.json\n",
      "INFO:convert_hf_to_gguf_update:smollm: File models/tokenizers/smollm/config.json already exists - skipping\n",
      "INFO:convert_hf_to_gguf_update:smollm: File models/tokenizers/smollm/tokenizer.json already exists - skipping\n",
      "INFO:convert_hf_to_gguf_update:smollm: File models/tokenizers/smollm/tokenizer_config.json already exists - skipping\n",
      "INFO:convert_hf_to_gguf_update:bloom: File models/tokenizers/bloom/config.json already exists - skipping\n",
      "INFO:convert_hf_to_gguf_update:bloom: File models/tokenizers/bloom/tokenizer.json already exists - skipping\n",
      "INFO:convert_hf_to_gguf_update:bloom: File models/tokenizers/bloom/tokenizer_config.json already exists - skipping\n",
      "INFO:convert_hf_to_gguf_update:gpt3-finnish: File models/tokenizers/gpt3-finnish/config.json already exists - skipping\n",
      "INFO:convert_hf_to_gguf_update:gpt3-finnish: File models/tokenizers/gpt3-finnish/tokenizer.json already exists - skipping\n",
      "INFO:convert_hf_to_gguf_update:gpt3-finnish: File models/tokenizers/gpt3-finnish/tokenizer_config.json already exists - skipping\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"GET /LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct/resolve/main/config.json HTTP/11\" 403 191\n",
      "ERROR:convert_hf_to_gguf_update:Failed to download model exaone. Error: 403 Client Error: Forbidden for url: https://huggingface.co/LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct/resolve/main/config.json\n",
      "ERROR:convert_hf_to_gguf_update:Error loading tokenizer for model llama-bpe. The model may not exist or is not accessible with the provided token. Error: models/tokenizers/llama-bpe does not appear to have a file named config.json. Checkout 'https://huggingface.co/models/tokenizers/llama-bpe/tree/None' for available files.\n",
      "INFO:convert_hf_to_gguf_update:model: deepseek-llm\n",
      "INFO:convert_hf_to_gguf_update:tokt: 2\n",
      "INFO:convert_hf_to_gguf_update:repo: https://huggingface.co/deepseek-ai/deepseek-llm-7b-base\n",
      "INFO:convert_hf_to_gguf_update:chktok: [100000, 185, 207, 185, 185, 207, 185, 185, 185, 207, 11969, 486, 22504, 185, 243, 185, 300, 185, 251, 185, 663, 185, 10044, 95300, 334, 8754, 8, 33701, 114, 350, 222, 10044, 221, 104, 46713, 334, 34732, 996, 24250, 262, 80923, 8, 207, 37103, 214, 12356, 99, 234, 10044, 99, 234, 207, 18, 207, 18, 18, 207, 18, 18, 18, 207, 18, 18, 18, 18, 207, 18, 18, 18, 18, 18, 207, 18, 18, 18, 18, 18, 18, 207, 18, 18, 18, 18, 18, 18, 18, 207, 18, 18, 18, 18, 18, 18, 18, 18, 207, 18, 13, 18, 207, 18, 526, 18, 207, 18, 1204, 18, 207, 71374, 209, 71374, 114, 71374, 228, 155, 240, 220, 71374, 224, 155, 240, 211, 71374, 231, 71374, 115, 71374, 240, 155, 240, 210, 71374, 240, 71374, 95, 71374, 114, 71374, 214, 71899, 210, 3025, 19017, 612, 9407, 2681, 16, 18, 16, 19, 16, 20, 16, 1398, 68940, 239, 78827, 55170, 76659, 620, 91754, 31116, 36804, 4885, 4885, 10897, 4390, 4390, 41047, 15278, 3033, 14986, 5675, 304, 6, 313, 803, 655, 33326, 362, 6, 82, 745, 11, 655, 1374, 340, 2049, 30, 655, 44, 441, 2049, 304, 6, 647, 1099, 359, 11, 655, 35, 340, 837, 742, 10842, 30, 1003, 6, 10699, 245, 6, 75, 43]\n",
      "INFO:convert_hf_to_gguf_update:chkhsh: 049ecf7629871e3041641907f3de7c733e4dbfdc736f57d882ba0b0845599754\n",
      "INFO:convert_hf_to_gguf_update:normalizer: {\n",
      "    \"type\": \"Sequence\",\n",
      "    \"normalizers\": []\n",
      "}\n",
      "INFO:convert_hf_to_gguf_update:pre_tokenizer: {\n",
      "    \"type\": \"Sequence\",\n",
      "    \"pretokenizers\": [\n",
      "        {\n",
      "            \"type\": \"Split\",\n",
      "            \"pattern\": {\n",
      "                \"Regex\": \"[\\r\\n]\"\n",
      "            },\n",
      "            \"behavior\": \"Isolated\",\n",
      "            \"invert\": false\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"Split\",\n",
      "            \"pattern\": {\n",
      "                \"Regex\": \"\\\\s?[A-Za-z\\u00b5\\u00c0-\\u00d6\\u00d8-\\u00f6\\u00f8-\\u01ba\\u01bc-\\u01bf\\u01c4-\\u0293\\u0295-\\u02af\\u0370-\\u0373\\u0376\\u0377\\u037b-\\u037d\\u037f\\u0386\\u0388-\\u038a\\u038c\\u038e-\\u03a1\\u03a3-\\u03f5\\u03f7-\\u0481\\u048a-\\u052f\\u0531-\\u0556\\u10a0-\\u10c5\\u13a0-\\u13f5\\u13f8-\\u13fd\\u1c90-\\u1cba\\u1cbd-\\u1cbf\\u1d00-\\u1d2b\\u1d6b-\\u1d77\\u1d79-\\u1d9a\\u1e00-\\u1f15\\u1f18-\\u1f1d\\u1f20-\\u1f45\\u1f48-\\u1f4d\\u1f50-\\u1f57\\u1f59\\u1f5b\\u1f5d\\u1f5f-\\u1f7d\\u1f80-\\u1fb4\\u1fb6-\\u1fbc\\u1fbe\\u1fc2-\\u1fc4\\u1fc6-\\u1fcc\\u1fd0-\\u1fd3\\u1fd6-\\u1fdb\\u1fe0-\\u1fec\\u1ff2-\\u1ff4\\u1ff6-\\u1ffc\\u2102\\u2107\\u210a-\\u2113\\u2115\\u2119-\\u211d\\u2124\\u2126\\u2128\\u212a-\\u212d\\u212f-\\u2134\\u2139\\u213c-\\u213f\\u2145-\\u2149\\u214e\\u2183\\u2184\\u2c00-\\u2c7b\\u2c7e-\\u2ce4\\u2ceb-\\u2cee\\u2cf2\\u2cf3\\ua640-\\ua66d\\ua680-\\ua69b\\ua722-\\ua76f\\ua771-\\ua787\\ua78b-\\ua78e\\uab70-\\uabbf\\ufb00-\\ufb06\\ufb13-\\ufb17\\uff21-\\uff3a\\uff41-\\uff5a\\ud801\\udc00-\\ud801\\udc4f\\ud801\\udcb0-\\ud801\\udcd3\\ud801\\udcd8-\\ud801\\udcfb\\ud803\\udc80-\\ud803\\udcb2\\ud803\\udcc0-\\ud803\\udcf2\\ud806\\udca0-\\ud806\\udcdf\\ud83a\\udd00-\\ud83a\\udd43]+\"\n",
      "            },\n",
      "            \"behavior\": \"Isolated\",\n",
      "            \"invert\": false\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"Split\",\n",
      "            \"pattern\": {\n",
      "                \"Regex\": \"\\\\s?[!-/:-~\\uff01-\\uff0f\\uff1a-\\uff5e\\u2018-\\u201f\\u3000-\\u3002]+\"\n",
      "            },\n",
      "            \"behavior\": \"Isolated\",\n",
      "            \"invert\": false\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"Split\",\n",
      "            \"pattern\": {\n",
      "                \"Regex\": \"\\\\s+$\"\n",
      "            },\n",
      "            \"behavior\": \"Isolated\",\n",
      "            \"invert\": false\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"Split\",\n",
      "            \"pattern\": {\n",
      "                \"Regex\": \"[\\u4e00-\\u9fa5\\u0800-\\u4e00\\uac00-\\ud7ff]+\"\n",
      "            },\n",
      "            \"behavior\": \"Isolated\",\n",
      "            \"invert\": false\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"Digits\",\n",
      "            \"individual_digits\": true\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"ByteLevel\",\n",
      "            \"add_prefix_space\": false,\n",
      "            \"trim_offsets\": true,\n",
      "            \"use_regex\": false\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "INFO:convert_hf_to_gguf_update:\n",
      "INFO:convert_hf_to_gguf_update:model: deepseek-coder\n",
      "INFO:convert_hf_to_gguf_update:tokt: 2\n",
      "INFO:convert_hf_to_gguf_update:repo: https://huggingface.co/deepseek-ai/deepseek-coder-6.7b-base\n",
      "INFO:convert_hf_to_gguf_update:chktok: [32013, 185, 207, 185, 185, 207, 185, 185, 185, 207, 12405, 459, 22758, 185, 243, 185, 315, 185, 251, 185, 730, 185, 10047, 235, 209, 334, 8760, 8, 12394, 233, 114, 350, 222, 10047, 221, 104, 169, 116, 224, 334, 4684, 3909, 992, 24330, 262, 29651, 612, 8, 207, 156, 237, 214, 12394, 99, 234, 10047, 99, 234, 207, 18, 207, 18, 18, 207, 18, 18, 18, 207, 18, 18, 18, 18, 207, 18, 18, 18, 18, 18, 207, 18, 18, 18, 18, 18, 18, 207, 18, 18, 18, 18, 18, 18, 18, 207, 18, 18, 18, 18, 18, 18, 18, 18, 207, 18, 13, 18, 207, 18, 524, 18, 207, 18, 1202, 18, 207, 155, 239, 209, 155, 239, 114, 155, 239, 228, 155, 240, 220, 155, 239, 224, 155, 240, 211, 155, 239, 231, 155, 239, 115, 155, 239, 240, 155, 240, 210, 155, 239, 240, 155, 239, 95, 155, 239, 114, 155, 239, 214, 10047, 233, 210, 3015, 19100, 608, 9413, 2668, 16, 18, 16, 19, 16, 20, 16, 1393, 169, 121, 239, 18155, 374, 17194, 28, 2861, 6478, 616, 2251, 14994, 31269, 4191, 6, 4686, 4686, 10252, 3358, 3358, 3409, 524, 15330, 3023, 15031, 5668, 303, 6, 312, 798, 651, 83, 839, 362, 6, 82, 741, 11, 651, 1369, 340, 2037, 30, 651, 44, 441, 2037, 303, 6, 642, 1098, 359, 11, 651, 35, 340, 833, 738, 10860, 30, 998, 6, 10709, 245, 6, 75, 43]\n",
      "INFO:convert_hf_to_gguf_update:chkhsh: 347715f544604f9118bb75ed199f68779f423cabb20db6de6f31b908d04d7821\n",
      "INFO:convert_hf_to_gguf_update:normalizer: {\n",
      "    \"type\": \"Sequence\",\n",
      "    \"normalizers\": []\n",
      "}\n",
      "INFO:convert_hf_to_gguf_update:pre_tokenizer: {\n",
      "    \"type\": \"Sequence\",\n",
      "    \"pretokenizers\": [\n",
      "        {\n",
      "            \"type\": \"Split\",\n",
      "            \"pattern\": {\n",
      "                \"Regex\": \"[\\r\\n]\"\n",
      "            },\n",
      "            \"behavior\": \"Isolated\",\n",
      "            \"invert\": false\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"Split\",\n",
      "            \"pattern\": {\n",
      "                \"Regex\": \"\\\\s?\\\\p{L}+\"\n",
      "            },\n",
      "            \"behavior\": \"Isolated\",\n",
      "            \"invert\": false\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"Split\",\n",
      "            \"pattern\": {\n",
      "                \"Regex\": \"\\\\s?\\\\p{P}+\"\n",
      "            },\n",
      "            \"behavior\": \"Isolated\",\n",
      "            \"invert\": false\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"Split\",\n",
      "            \"pattern\": {\n",
      "                \"Regex\": \"[\\u4e00-\\u9fa5\\u0800-\\u4e00\\uac00-\\ud7ff]+\"\n",
      "            },\n",
      "            \"behavior\": \"Isolated\",\n",
      "            \"invert\": false\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"Digits\",\n",
      "            \"individual_digits\": true\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"ByteLevel\",\n",
      "            \"add_prefix_space\": false,\n",
      "            \"trim_offsets\": true,\n",
      "            \"use_regex\": false\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "INFO:convert_hf_to_gguf_update:\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "INFO:convert_hf_to_gguf_update:model: falcon\n",
      "INFO:convert_hf_to_gguf_update:tokt: 2\n",
      "INFO:convert_hf_to_gguf_update:repo: https://huggingface.co/tiiuae/falcon-7b\n",
      "INFO:convert_hf_to_gguf_update:chktok: [1212, 4824, 1001, 1212, 192, 204, 663, 49453, 2069, 742, 561, 1501, 193, 2571, 232, 206, 204, 19, 11003, 20, 8196, 126, 283, 219, 48778, 116, 13392, 204, 19, 51831, 732, 63209, 1741, 7955, 522, 20, 22438, 211, 3346, 111, 231, 2571, 111, 231, 204, 30, 204, 3138, 204, 22287, 204, 22287, 30, 204, 22287, 3138, 204, 22287, 22287, 204, 22287, 22287, 30, 204, 22287, 22287, 3138, 204, 30, 25, 30, 204, 30, 513, 30, 204, 30, 951, 30, 27171, 236, 206, 38154, 126, 38154, 225, 167, 237, 217, 38154, 221, 167, 237, 208, 38154, 228, 38154, 127, 38154, 237, 167, 237, 207, 38154, 237, 38154, 107, 38154, 126, 38154, 211, 20589, 207, 204, 42, 50087, 123, 2727, 20300, 32022, 133, 234, 17419, 30137, 28, 7858, 181, 133, 236, 204, 37057, 2228, 10666, 5052, 133, 6207, 151, 215, 150, 134, 5052, 133, 6279, 5052, 223, 151, 216, 49679, 123, 53110, 47043, 7795, 204, 7544, 7544, 7544, 8543, 8543, 17593, 3513, 3513, 12844, 51520, 17664, 4247, 295, 18, 298, 650, 204, 18, 95, 693, 332, 18, 94, 629, 23, 204, 18, 1553, 299, 1310, 42, 204, 18, 56, 416, 1310, 295, 18, 567, 717, 334, 23, 204, 18, 47, 299, 606, 596, 6696, 42, 703, 18, 16139, 241, 18, 87, 55]\n",
      "INFO:convert_hf_to_gguf_update:chkhsh: 8aeee3860c56296a157a1fe2fad249ec40aa59b1bb5709f4ade11c4e6fe652ed\n",
      "INFO:convert_hf_to_gguf_update:normalizer: null\n",
      "INFO:convert_hf_to_gguf_update:pre_tokenizer: {\n",
      "    \"type\": \"Sequence\",\n",
      "    \"pretokenizers\": [\n",
      "        {\n",
      "            \"type\": \"Punctuation\",\n",
      "            \"behavior\": \"Contiguous\"\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"ByteLevel\",\n",
      "            \"add_prefix_space\": false,\n",
      "            \"trim_offsets\": true,\n",
      "            \"use_regex\": true\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"Digits\",\n",
      "            \"individual_digits\": false\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"Split\",\n",
      "            \"pattern\": {\n",
      "                \"Regex\": \"[0-9][0-9][0-9]\"\n",
      "            },\n",
      "            \"behavior\": \"Isolated\",\n",
      "            \"invert\": false\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "INFO:convert_hf_to_gguf_update:\n",
      "INFO:convert_hf_to_gguf_update:model: bert-bge\n",
      "INFO:convert_hf_to_gguf_update:tokt: 3\n",
      "INFO:convert_hf_to_gguf_update:repo: https://huggingface.co/BAAI/bge-small-en-v1.5\n",
      "INFO:convert_hf_to_gguf_update:chktok: [101, 100, 1006, 3671, 1007, 100, 1006, 3674, 7861, 29147, 2483, 9530, 16280, 23854, 1007, 100, 100, 1017, 3943, 21211, 21211, 2509, 21211, 22394, 21211, 22394, 2509, 21211, 22394, 22394, 21211, 22394, 22394, 2509, 1017, 1012, 1017, 1017, 1012, 1012, 1017, 1017, 1012, 1012, 1012, 1017, 100, 1029, 1855, 100, 100, 6207, 100, 100, 14677, 23632, 22203, 1811, 1995, 1011, 1011, 1011, 1011, 1011, 1011, 1027, 1027, 1027, 1027, 1027, 1027, 1027, 1192, 15290, 29754, 14150, 1192, 10260, 1181, 29755, 29436, 29741, 10260, 16856, 29747, 23925, 10325, 1005, 1005, 1005, 1005, 1005, 1005, 1036, 1036, 1036, 1036, 1036, 1036, 1036, 1000, 1000, 1000, 1000, 1012, 1012, 1012, 1012, 1012, 1012, 999, 999, 999, 999, 999, 999, 1029, 1029, 1029, 1029, 1029, 1029, 1045, 1005, 2310, 2042, 1005, 2409, 2002, 1005, 1055, 2045, 1010, 1005, 2128, 2017, 2469, 1029, 1005, 1049, 2025, 2469, 1045, 1005, 2222, 2191, 2009, 1010, 1005, 1040, 2017, 2066, 2070, 5572, 1029, 2057, 1005, 2310, 1037, 1005, 2222, 102]\n",
      "INFO:convert_hf_to_gguf_update:chkhsh: 0876d13b50744004aa9aeae05e7b0647eac9d801b5ba4668afc01e709c15e19f\n",
      "INFO:convert_hf_to_gguf_update:normalizer: {\n",
      "    \"type\": \"BertNormalizer\",\n",
      "    \"clean_text\": true,\n",
      "    \"handle_chinese_chars\": true,\n",
      "    \"strip_accents\": null,\n",
      "    \"lowercase\": true\n",
      "}\n",
      "INFO:convert_hf_to_gguf_update:pre_tokenizer: {\n",
      "    \"type\": \"BertPreTokenizer\"\n",
      "}\n",
      "INFO:convert_hf_to_gguf_update:\n",
      "INFO:convert_hf_to_gguf_update:model: mpt\n",
      "INFO:convert_hf_to_gguf_update:tokt: 2\n",
      "INFO:convert_hf_to_gguf_update:repo: https://huggingface.co/mosaicml/mpt-7b\n",
      "INFO:convert_hf_to_gguf_update:chktok: [586, 1744, 33525, 186, 209, 623, 28910, 187, 50276, 187, 50275, 187, 50274, 187, 50273, 187, 14931, 237, 211, 313, 6320, 10, 49042, 116, 325, 224, 14931, 223, 106, 171, 118, 226, 313, 34263, 802, 13511, 261, 32147, 456, 10, 3384, 239, 216, 22692, 101, 236, 14931, 101, 236, 495, 5922, 30057, 495, 20084, 495, 26409, 30057, 20084, 495, 26409, 1610, 495, 26409, 20084, 495, 15, 20, 495, 537, 20, 495, 1051, 20, 209, 18081, 211, 18081, 116, 18081, 230, 39936, 222, 18081, 226, 39936, 213, 18081, 233, 18081, 117, 18081, 242, 39936, 212, 18081, 242, 18081, 97, 18081, 116, 18081, 216, 14931, 235, 212, 3736, 15367, 41197, 13610, 19934, 41869, 21275, 1012, 1047, 18795, 40120, 20422, 241, 16081, 6877, 12880, 11514, 1068, 8713, 38177, 13396, 3415, 9925, 12559, 10453, 1389, 42011, 35033, 34842, 11202, 9739, 9739, 33021, 18963, 4672, 25561, 8220, 309, 1849, 644, 686, 42618, 344, 434, 627, 13, 686, 1848, 368, 2119, 32, 686, 46, 417, 2119, 309, 1833, 1056, 352, 13, 686, 37, 368, 751, 690, 10331, 32, 844, 8, 31516, 247, 8, 77, 45]\n",
      "INFO:convert_hf_to_gguf_update:chkhsh: b6dc8df998e1cfbdc4eac8243701a65afe638679230920b50d6f17d81c098166\n",
      "INFO:convert_hf_to_gguf_update:normalizer: {\n",
      "    \"type\": \"NFC\"\n",
      "}\n",
      "INFO:convert_hf_to_gguf_update:pre_tokenizer: {\n",
      "    \"type\": \"ByteLevel\",\n",
      "    \"add_prefix_space\": false,\n",
      "    \"trim_offsets\": true,\n",
      "    \"use_regex\": true\n",
      "}\n",
      "INFO:convert_hf_to_gguf_update:\n",
      "INFO:convert_hf_to_gguf_update:model: starcoder\n",
      "INFO:convert_hf_to_gguf_update:tokt: 2\n",
      "INFO:convert_hf_to_gguf_update:repo: https://huggingface.co/bigcode/starcoder2-3b\n",
      "INFO:convert_hf_to_gguf_update:chktok: [353, 736, 8886, 221, 10883, 4238, 16101, 28540, 222, 3822, 272, 246, 327, 4434, 46, 18445, 152, 46030, 45022, 142, 13878, 327, 12585, 19884, 33773, 40920, 751, 46, 41839, 5954, 137, 271, 3822, 137, 271, 244, 56, 244, 56, 56, 244, 56, 56, 56, 244, 56, 56, 56, 56, 244, 56, 56, 56, 56, 56, 244, 56, 56, 56, 56, 56, 56, 244, 56, 56, 56, 56, 56, 56, 56, 244, 56, 56, 56, 56, 56, 56, 56, 56, 244, 56, 51, 56, 244, 56, 516, 56, 244, 56, 1198, 56, 244, 14566, 246, 14566, 152, 14566, 265, 30428, 257, 14566, 261, 30428, 248, 14566, 268, 14566, 153, 14566, 277, 30428, 247, 14566, 277, 14566, 133, 14566, 152, 14566, 251, 36570, 247, 1037, 4995, 13379, 2924, 9515, 17823, 54, 56, 54, 57, 54, 58, 54, 11904, 47892, 20895, 16625, 13047, 8389, 1059, 9504, 40216, 13858, 2073, 8983, 12571, 1539, 10721, 5918, 9643, 13298, 932, 31723, 31330, 9221, 3226, 35426, 10400, 457, 4783, 2602, 349, 121, 1477, 957, 1200, 2038, 49, 349, 632, 863, 3673, 68, 349, 82, 666, 3673, 457, 4650, 1949, 580, 49, 349, 73, 863, 2144, 1649, 35941, 68, 2726, 44, 7728, 331, 44, 113, 81]\n",
      "INFO:convert_hf_to_gguf_update:chkhsh: 35d91631860c815f952d711435f48d356ebac988362536bed955d43bfa436e34\n",
      "INFO:convert_hf_to_gguf_update:normalizer: null\n",
      "INFO:convert_hf_to_gguf_update:pre_tokenizer: {\n",
      "    \"type\": \"Sequence\",\n",
      "    \"pretokenizers\": [\n",
      "        {\n",
      "            \"type\": \"Digits\",\n",
      "            \"individual_digits\": true\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"ByteLevel\",\n",
      "            \"add_prefix_space\": false,\n",
      "            \"trim_offsets\": true,\n",
      "            \"use_regex\": true\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "INFO:convert_hf_to_gguf_update:\n",
      "INFO:convert_hf_to_gguf_update:model: gpt-2\n",
      "INFO:convert_hf_to_gguf_update:tokt: 2\n",
      "INFO:convert_hf_to_gguf_update:repo: https://huggingface.co/openai-community/gpt2\n",
      "INFO:convert_hf_to_gguf_update:chktok: [198, 220, 628, 220, 628, 198, 220, 197, 220, 197, 197, 220, 197, 198, 220, 220, 198, 220, 220, 220, 198, 220, 220, 220, 220, 198, 220, 220, 220, 220, 220, 198, 8582, 248, 222, 357, 11265, 8, 30325, 114, 447, 235, 8582, 234, 104, 37929, 357, 48101, 795, 13210, 271, 1673, 36686, 515, 8, 14519, 227, 12520, 99, 247, 8582, 99, 247, 513, 4747, 23460, 513, 20370, 23460, 2091, 23460, 20370, 23460, 24840, 23460, 2091, 20370, 513, 13, 18, 513, 492, 18, 513, 986, 18, 28053, 252, 222, 157, 252, 114, 157, 252, 241, 157, 253, 233, 157, 252, 237, 157, 253, 224, 157, 252, 244, 157, 252, 115, 157, 252, 253, 157, 253, 223, 157, 252, 253, 157, 252, 95, 157, 252, 114, 157, 252, 227, 47249, 223, 5633, 22755, 239, 46349, 111, 28839, 101, 18040, 32432, 98, 43291, 1485, 1415, 24309, 25465, 171, 121, 252, 40103, 1421, 18604, 12466, 121, 16843, 141, 231, 15166, 12466, 121, 16142, 12466, 239, 141, 232, 30143, 140, 111, 16142, 21169, 21727, 31583, 18849, 705, 39115, 6, 33153, 15506, 63, 15931, 15931, 16317, 13896, 3228, 9805, 3548, 314, 1053, 587, 705, 44040, 339, 338, 612, 11, 705, 2200, 345, 1654, 30, 705, 44, 407, 1654, 314, 1183, 787, 340, 11, 705, 35, 345, 588, 617, 8887, 30, 775, 6, 26979, 257, 6, 75, 43]\n",
      "INFO:convert_hf_to_gguf_update:chkhsh: 3ce83efda5659b07b1ad37ca97ca5797ea4285d9b9ab0dc679e4a720c9da7454\n",
      "INFO:convert_hf_to_gguf_update:normalizer: null\n",
      "INFO:convert_hf_to_gguf_update:pre_tokenizer: {\n",
      "    \"type\": \"ByteLevel\",\n",
      "    \"add_prefix_space\": false,\n",
      "    \"trim_offsets\": true\n",
      "}\n",
      "INFO:convert_hf_to_gguf_update:\n",
      "INFO:convert_hf_to_gguf_update:model: stablelm2\n",
      "INFO:convert_hf_to_gguf_update:tokt: 2\n",
      "INFO:convert_hf_to_gguf_update:repo: https://huggingface.co/stabilityai/stablelm-2-zephyr-1_6b\n",
      "INFO:convert_hf_to_gguf_update:chktok: [198, 4815, 15073, 66597, 8004, 1602, 2355, 79772, 11187, 9468, 248, 222, 320, 8416, 8, 27623, 114, 378, 235, 9468, 234, 104, 31643, 320, 36773, 100166, 98634, 8, 26602, 227, 11410, 99, 247, 9468, 99, 247, 220, 18, 220, 18, 18, 220, 18, 18, 18, 220, 18, 18, 18, 18, 220, 18, 18, 18, 18, 18, 220, 18, 18, 18, 18, 18, 18, 220, 18, 18, 18, 18, 18, 18, 18, 220, 18, 18, 18, 18, 18, 18, 18, 18, 220, 18, 13, 18, 220, 18, 497, 18, 220, 18, 1131, 18, 220, 21549, 222, 98629, 241, 45358, 233, 21549, 237, 45358, 224, 21549, 244, 21549, 115, 21549, 253, 45358, 223, 21549, 253, 21549, 95, 98629, 227, 76460, 223, 949, 37046, 33565, 111, 19000, 23182, 49792, 19967, 16, 18, 16, 19, 16, 20, 16, 36827, 21909, 56560, 54337, 19175, 14476, 1482, 13373, 64571, 34694, 3114, 15752, 17721, 80112, 3436, 4708, 4708, 14196, 14196, 74694, 3089, 3089, 29249, 17523, 3001, 27708, 7801, 358, 3077, 1027, 364, 83, 820, 568, 596, 1070, 11, 364, 793, 499, 2771, 30, 364, 44, 539, 2771, 358, 3358, 1304, 433, 11, 364, 35, 499, 1093, 1063, 15600, 30, 1226, 6, 43712, 264, 64966, 43]\n",
      "INFO:convert_hf_to_gguf_update:chkhsh: 32d85c31273f8019248f2559fed492d929ea28b17e51d81d3bb36fff23ca72b3\n",
      "INFO:convert_hf_to_gguf_update:normalizer: null\n",
      "INFO:convert_hf_to_gguf_update:pre_tokenizer: {\n",
      "    \"type\": \"Sequence\",\n",
      "    \"pretokenizers\": [\n",
      "        {\n",
      "            \"type\": \"Split\",\n",
      "            \"pattern\": {\n",
      "                \"Regex\": \"(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\\\p{L}\\\\p{N}]?\\\\p{L}+|\\\\p{N}| ?[^\\\\s\\\\p{L}\\\\p{N}]+[\\r\\n]*|\\\\s*[\\r\\n]+|\\\\s+(?!\\\\S)|\\\\s+\"\n",
      "            },\n",
      "            \"behavior\": \"Removed\",\n",
      "            \"invert\": true\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"ByteLevel\",\n",
      "            \"add_prefix_space\": false,\n",
      "            \"trim_offsets\": true,\n",
      "            \"use_regex\": false\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "INFO:convert_hf_to_gguf_update:\n",
      "INFO:convert_hf_to_gguf_update:model: refact\n",
      "INFO:convert_hf_to_gguf_update:tokt: 2\n",
      "INFO:convert_hf_to_gguf_update:repo: https://huggingface.co/smallcloudai/Refact-1_6-base\n",
      "INFO:convert_hf_to_gguf_update:chktok: [334, 719, 8878, 202, 10885, 4222, 16104, 28570, 203, 3807, 253, 227, 308, 4382, 27, 18458, 133, 46113, 44967, 123, 13868, 308, 12565, 19775, 33071, 40824, 733, 27, 41889, 5945, 118, 252, 3807, 118, 252, 225, 37, 225, 37, 37, 225, 37, 37, 37, 225, 37, 37, 37, 37, 225, 37, 37, 37, 37, 37, 225, 37, 37, 37, 37, 37, 37, 225, 37, 37, 37, 37, 37, 37, 37, 225, 37, 37, 37, 37, 37, 37, 37, 37, 225, 37, 32, 37, 225, 37, 497, 37, 225, 37, 1179, 37, 225, 14574, 227, 14574, 133, 14574, 246, 30457, 238, 14574, 242, 30457, 229, 14574, 249, 14574, 134, 14574, 258, 30457, 228, 14574, 258, 14574, 114, 14574, 133, 14574, 232, 36628, 228, 1018, 4982, 13368, 2909, 9513, 17827, 35, 37, 35, 38, 35, 39, 35, 11873, 47838, 20921, 16623, 13028, 8372, 1039, 9446, 40242, 13852, 2053, 8949, 12531, 1520, 10700, 5881, 9592, 13299, 914, 31753, 31359, 9163, 3202, 35472, 10397, 439, 4763, 2583, 330, 102, 1455, 938, 1182, 2017, 30, 330, 613, 844, 3654, 49, 330, 63, 646, 3654, 439, 4621, 1930, 561, 30, 330, 54, 844, 2124, 1629, 35993, 49, 2688, 25, 7709, 312, 25, 94, 62]\n",
      "INFO:convert_hf_to_gguf_update:chkhsh: 6221ad2852e85ce96f791f476e0b390cf9b474c9e3d1362f53a24a06dc8220ff\n",
      "INFO:convert_hf_to_gguf_update:normalizer: null\n",
      "INFO:convert_hf_to_gguf_update:pre_tokenizer: {\n",
      "    \"type\": \"Sequence\",\n",
      "    \"pretokenizers\": [\n",
      "        {\n",
      "            \"type\": \"Digits\",\n",
      "            \"individual_digits\": true\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"ByteLevel\",\n",
      "            \"add_prefix_space\": false,\n",
      "            \"trim_offsets\": true,\n",
      "            \"use_regex\": true\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "INFO:convert_hf_to_gguf_update:\n",
      "ERROR:convert_hf_to_gguf_update:Error loading tokenizer for model command-r. The model may not exist or is not accessible with the provided token. Error: models/tokenizers/command-r does not appear to have a file named config.json. Checkout 'https://huggingface.co/models/tokenizers/command-r/tree/None' for available files.\n",
      "INFO:convert_hf_to_gguf_update:model: qwen2\n",
      "INFO:convert_hf_to_gguf_update:tokt: 2\n",
      "INFO:convert_hf_to_gguf_update:repo: https://huggingface.co/Qwen/Qwen1.5-7B\n",
      "INFO:convert_hf_to_gguf_update:chktok: [198, 4710, 14731, 65497, 7847, 1572, 2303, 78672, 10947, 145836, 320, 8252, 8, 26525, 114, 378, 235, 149921, 30543, 320, 35673, 99066, 97534, 8, 25521, 227, 11162, 99, 247, 149955, 220, 18, 220, 18, 18, 220, 18, 18, 18, 220, 18, 18, 18, 18, 220, 18, 18, 18, 18, 18, 220, 18, 18, 18, 18, 18, 18, 220, 18, 18, 18, 18, 18, 18, 18, 220, 18, 18, 18, 18, 18, 18, 18, 18, 220, 18, 13, 18, 220, 18, 496, 18, 220, 18, 1112, 18, 220, 146394, 97529, 241, 44258, 233, 146568, 44258, 224, 147603, 20879, 115, 146280, 44258, 223, 146280, 147272, 97529, 227, 144534, 937, 104100, 18493, 22377, 99257, 16, 18, 16, 19, 16, 20, 16, 35727, 21216, 55460, 53237, 18658, 14144, 1456, 13073, 63471, 33594, 3038, 133178, 79012, 3355, 4605, 4605, 13874, 13874, 73594, 3014, 3014, 28149, 17085, 2928, 26610, 7646, 358, 3003, 1012, 364, 83, 813, 566, 594, 1052, 11, 364, 787, 498, 2704, 30, 364, 44, 537, 2704, 358, 3278, 1281, 432, 11, 364, 35, 498, 1075, 1045, 15243, 30, 1205, 6, 42612, 264, 63866, 43]\n",
      "INFO:convert_hf_to_gguf_update:chkhsh: e636dc30a262dcc0d8c323492e32ae2b70728f4df7dfe9737d9f920a282b8aea\n",
      "INFO:convert_hf_to_gguf_update:normalizer: {\n",
      "    \"type\": \"NFC\"\n",
      "}\n",
      "INFO:convert_hf_to_gguf_update:pre_tokenizer: {\n",
      "    \"type\": \"Sequence\",\n",
      "    \"pretokenizers\": [\n",
      "        {\n",
      "            \"type\": \"Split\",\n",
      "            \"pattern\": {\n",
      "                \"Regex\": \"(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\\\r\\\\n\\\\p{L}\\\\p{N}]?\\\\p{L}+|\\\\p{N}| ?[^\\\\s\\\\p{L}\\\\p{N}]+[\\\\r\\\\n]*|\\\\s*[\\\\r\\\\n]+|\\\\s+(?!\\\\S)|\\\\s+\"\n",
      "            },\n",
      "            \"behavior\": \"Isolated\",\n",
      "            \"invert\": false\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"ByteLevel\",\n",
      "            \"add_prefix_space\": false,\n",
      "            \"trim_offsets\": false,\n",
      "            \"use_regex\": false\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "INFO:convert_hf_to_gguf_update:\n",
      "INFO:convert_hf_to_gguf_update:model: olmo\n",
      "INFO:convert_hf_to_gguf_update:tokt: 2\n",
      "INFO:convert_hf_to_gguf_update:repo: https://huggingface.co/allenai/OLMo-1.7-7B-hf\n",
      "INFO:convert_hf_to_gguf_update:chktok: [586, 1744, 33525, 186, 209, 623, 28910, 187, 50276, 187, 50275, 187, 50274, 187, 50273, 187, 14931, 237, 211, 313, 6320, 10, 49042, 116, 325, 224, 14931, 223, 106, 171, 118, 226, 313, 34263, 802, 13511, 261, 32147, 456, 10, 3384, 239, 216, 22692, 101, 236, 14931, 101, 236, 495, 5922, 30057, 495, 20084, 495, 26409, 30057, 20084, 495, 26409, 1610, 495, 26409, 20084, 495, 15, 20, 495, 537, 20, 495, 1051, 20, 209, 18081, 211, 18081, 116, 18081, 230, 39936, 222, 18081, 226, 39936, 213, 18081, 233, 18081, 117, 18081, 242, 39936, 212, 18081, 242, 18081, 97, 18081, 116, 18081, 216, 14931, 235, 212, 3736, 15367, 41197, 13610, 19934, 41869, 21275, 1012, 1047, 18795, 40120, 20422, 241, 16081, 6877, 12880, 11514, 1068, 8713, 38177, 13396, 3415, 9925, 12559, 10453, 1389, 42011, 35033, 34842, 11202, 9739, 9739, 33021, 18963, 4672, 25561, 8220, 309, 1849, 644, 686, 42618, 344, 434, 627, 13, 686, 1848, 368, 2119, 32, 686, 46, 417, 2119, 309, 1833, 1056, 352, 13, 686, 37, 368, 751, 690, 10331, 32, 844, 8, 31516, 247, 8, 77, 45]\n",
      "INFO:convert_hf_to_gguf_update:chkhsh: b6dc8df998e1cfbdc4eac8243701a65afe638679230920b50d6f17d81c098166\n",
      "INFO:convert_hf_to_gguf_update:normalizer: {\n",
      "    \"type\": \"NFC\"\n",
      "}\n",
      "INFO:convert_hf_to_gguf_update:pre_tokenizer: {\n",
      "    \"type\": \"ByteLevel\",\n",
      "    \"add_prefix_space\": false,\n",
      "    \"trim_offsets\": true,\n",
      "    \"use_regex\": true\n",
      "}\n",
      "INFO:convert_hf_to_gguf_update:\n",
      "ERROR:convert_hf_to_gguf_update:Error loading tokenizer for model dbrx. The model may not exist or is not accessible with the provided token. Error: models/tokenizers/dbrx does not appear to have a file named config.json. Checkout 'https://huggingface.co/models/tokenizers/dbrx/tree/None' for available files.\n",
      "INFO:convert_hf_to_gguf_update:model: jina-v2-en\n",
      "INFO:convert_hf_to_gguf_update:tokt: 3\n",
      "INFO:convert_hf_to_gguf_update:repo: https://huggingface.co/jinaai/jina-embeddings-v2-base-en\n",
      "INFO:convert_hf_to_gguf_update:chktok: [101, 100, 1006, 3671, 1007, 100, 1006, 3674, 7861, 29147, 2483, 9530, 16280, 23854, 1007, 100, 100, 1017, 3943, 21211, 21211, 2509, 21211, 22394, 21211, 22394, 2509, 21211, 22394, 22394, 21211, 22394, 22394, 2509, 1017, 1012, 1017, 1017, 1012, 1012, 1017, 1017, 1012, 1012, 1012, 1017, 100, 1029, 1855, 100, 100, 6207, 100, 100, 14677, 23632, 22203, 1811, 1995, 1011, 1011, 1011, 1011, 1011, 1011, 1027, 1027, 1027, 1027, 1027, 1027, 1027, 1192, 15290, 29754, 14150, 1192, 10260, 1181, 29755, 29436, 29741, 10260, 16856, 29747, 23925, 10325, 1005, 1005, 1005, 1005, 1005, 1005, 1036, 1036, 1036, 1036, 1036, 1036, 1036, 1000, 1000, 1000, 1000, 1012, 1012, 1012, 1012, 1012, 1012, 999, 999, 999, 999, 999, 999, 1029, 1029, 1029, 1029, 1029, 1029, 1045, 1005, 2310, 2042, 1005, 2409, 2002, 1005, 1055, 2045, 1010, 1005, 2128, 2017, 2469, 1029, 1005, 1049, 2025, 2469, 1045, 1005, 2222, 2191, 2009, 1010, 1005, 1040, 2017, 2066, 2070, 5572, 1029, 2057, 1005, 2310, 1037, 1005, 2222, 102]\n",
      "INFO:convert_hf_to_gguf_update:chkhsh: 0876d13b50744004aa9aeae05e7b0647eac9d801b5ba4668afc01e709c15e19f\n",
      "INFO:convert_hf_to_gguf_update:normalizer: {\n",
      "    \"type\": \"BertNormalizer\",\n",
      "    \"clean_text\": true,\n",
      "    \"handle_chinese_chars\": true,\n",
      "    \"strip_accents\": null,\n",
      "    \"lowercase\": true\n",
      "}\n",
      "INFO:convert_hf_to_gguf_update:pre_tokenizer: {\n",
      "    \"type\": \"BertPreTokenizer\"\n",
      "}\n",
      "INFO:convert_hf_to_gguf_update:\n",
      "INFO:convert_hf_to_gguf_update:model: jina-v2-es\n",
      "INFO:convert_hf_to_gguf_update:tokt: 2\n",
      "INFO:convert_hf_to_gguf_update:repo: https://huggingface.co/jinaai/jina-embeddings-v2-base-es\n",
      "INFO:convert_hf_to_gguf_update:chktok: [0, 203, 225, 203, 203, 225, 203, 203, 203, 225, 202, 225, 202, 202, 225, 202, 203, 7630, 203, 55433, 203, 15175, 203, 15175, 225, 203, 3555, 253, 227, 411, 20493, 13, 13714, 119, 44112, 53112, 109, 15190, 411, 36936, 3937, 47914, 284, 322, 12366, 263, 673, 13, 54349, 5829, 104, 252, 3555, 104, 252, 592, 8325, 58069, 592, 44717, 58069, 4147, 58069, 44717, 58069, 4147, 4147, 58069, 4147, 44717, 592, 18, 23, 592, 446, 23, 592, 652, 23, 34150, 257, 227, 162, 257, 119, 162, 257, 246, 162, 258, 238, 162, 257, 242, 162, 258, 229, 162, 257, 249, 162, 257, 120, 162, 257, 258, 162, 258, 228, 162, 257, 258, 162, 257, 100, 162, 257, 119, 162, 257, 232, 25524, 228, 6997, 60099, 244, 167, 230, 116, 53535, 106, 32992, 166, 120, 103, 42712, 255, 2514, 2445, 50542, 38690, 107, 176, 126, 257, 556, 6915, 17, 30705, 15386, 33, 41633, 7742, 146, 236, 6866, 41633, 7340, 5600, 244, 146, 237, 14787, 45142, 44845, 37316, 8267, 1049, 11483, 11483, 11, 47179, 47179, 47179, 68, 30218, 30218, 21797, 52975, 18077, 4232, 346, 1999, 904, 1049, 88, 1100, 551, 485, 862, 16, 1049, 2004, 368, 2124, 35, 1049, 49, 529, 2124, 346, 2178, 1064, 429, 16, 1049, 40, 368, 880, 848, 11196, 35, 1045, 11, 32698, 261, 11, 80, 48, 2]\n",
      "INFO:convert_hf_to_gguf_update:chkhsh: 171aeeedd6fb548d418a7461d053f11b6f1f1fc9b387bd66640d28a4b9f5c643\n",
      "INFO:convert_hf_to_gguf_update:normalizer: null\n",
      "INFO:convert_hf_to_gguf_update:pre_tokenizer: {\n",
      "    \"type\": \"ByteLevel\",\n",
      "    \"add_prefix_space\": false,\n",
      "    \"trim_offsets\": true,\n",
      "    \"use_regex\": true\n",
      "}\n",
      "INFO:convert_hf_to_gguf_update:\n",
      "INFO:convert_hf_to_gguf_update:model: jina-v2-de\n",
      "INFO:convert_hf_to_gguf_update:tokt: 2\n",
      "INFO:convert_hf_to_gguf_update:repo: https://huggingface.co/jinaai/jina-embeddings-v2-base-de\n",
      "INFO:convert_hf_to_gguf_update:chktok: [0, 203, 225, 203, 203, 225, 203, 203, 203, 225, 202, 225, 202, 202, 225, 202, 203, 6733, 203, 53448, 203, 13607, 203, 13607, 225, 203, 3753, 253, 227, 406, 17453, 13, 10278, 119, 54678, 3753, 239, 109, 16598, 406, 52806, 1504, 5752, 78, 276, 2365, 851, 697, 13, 38607, 5060, 104, 252, 3753, 104, 252, 589, 8235, 54381, 589, 45768, 54381, 3837, 54381, 45768, 54381, 3837, 3837, 54381, 3837, 45768, 589, 18, 23, 589, 466, 23, 589, 714, 23, 34376, 257, 227, 162, 257, 119, 162, 257, 246, 162, 258, 238, 162, 257, 242, 162, 258, 229, 162, 257, 249, 162, 257, 120, 162, 257, 258, 162, 258, 228, 162, 257, 258, 162, 257, 100, 162, 257, 119, 162, 257, 232, 32164, 228, 4985, 167, 235, 244, 167, 230, 116, 57520, 106, 33974, 166, 120, 103, 46520, 255, 2281, 2237, 42047, 47551, 107, 176, 126, 257, 485, 6624, 17, 30007, 14589, 33, 36028, 6983, 146, 236, 6294, 52261, 4933, 244, 146, 237, 13905, 32390, 46632, 51078, 1268, 12228, 12228, 11, 51396, 51396, 51396, 68, 30699, 30699, 21828, 11344, 1844, 20800, 4300, 324, 1990, 927, 1268, 88, 939, 540, 507, 899, 16, 1268, 3136, 426, 2158, 35, 1268, 49, 586, 2158, 324, 2202, 1066, 436, 16, 1268, 40, 426, 917, 822, 11788, 35, 628, 11, 30868, 264, 11, 80, 48, 2]\n",
      "INFO:convert_hf_to_gguf_update:chkhsh: 27949a2493fc4a9f53f5b9b029c82689cfbe5d3a1929bb25e043089e28466de6\n",
      "INFO:convert_hf_to_gguf_update:normalizer: null\n",
      "INFO:convert_hf_to_gguf_update:pre_tokenizer: {\n",
      "    \"type\": \"ByteLevel\",\n",
      "    \"add_prefix_space\": false,\n",
      "    \"trim_offsets\": true,\n",
      "    \"use_regex\": true\n",
      "}\n",
      "INFO:convert_hf_to_gguf_update:\n",
      "INFO:convert_hf_to_gguf_update:model: smaug-bpe\n",
      "INFO:convert_hf_to_gguf_update:tokt: 2\n",
      "INFO:convert_hf_to_gguf_update:repo: https://huggingface.co/abacusai/Smaug-Llama-3-70B-Instruct\n",
      "INFO:convert_hf_to_gguf_update:chktok: [198, 4815, 15073, 66597, 8004, 1602, 2355, 79772, 11187, 9468, 248, 222, 320, 8416, 8, 27623, 114, 102470, 9468, 234, 104, 31643, 320, 36773, 100166, 98634, 8, 26602, 227, 11410, 99, 247, 9468, 99, 247, 220, 18, 220, 1644, 220, 8765, 220, 8765, 18, 220, 8765, 1644, 220, 8765, 8765, 220, 8765, 8765, 18, 220, 8765, 8765, 1644, 220, 18, 13, 18, 220, 18, 497, 18, 220, 18, 1131, 18, 220, 21549, 222, 98629, 241, 45358, 233, 21549, 237, 45358, 224, 21549, 244, 21549, 115, 21549, 253, 45358, 223, 21549, 253, 21549, 95, 98629, 227, 76460, 223, 949, 37046, 101067, 19000, 23182, 102301, 9263, 18136, 16, 36827, 21909, 56560, 54337, 19175, 102118, 13373, 64571, 34694, 3114, 112203, 80112, 3436, 106451, 14196, 14196, 74694, 3089, 3089, 29249, 17523, 3001, 27708, 7801, 358, 3077, 1027, 364, 83, 820, 568, 596, 1070, 11, 364, 793, 499, 2771, 30, 364, 44, 539, 2771, 358, 3358, 1304, 433, 11, 364, 35, 499, 1093, 1063, 15600, 30, 1226, 6, 43712, 264, 64966, 43]\n",
      "INFO:convert_hf_to_gguf_update:chkhsh: c136ed14d01c2745d4f60a9596ae66800e2b61fa45643e72436041855ad4089d\n",
      "INFO:convert_hf_to_gguf_update:normalizer: null\n",
      "INFO:convert_hf_to_gguf_update:pre_tokenizer: {\n",
      "    \"type\": \"Sequence\",\n",
      "    \"pretokenizers\": [\n",
      "        {\n",
      "            \"type\": \"Split\",\n",
      "            \"pattern\": {\n",
      "                \"Regex\": \"(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\\\r\\\\n\\\\p{L}\\\\p{N}]?\\\\p{L}+|\\\\p{N}{1,3}| ?[^\\\\s\\\\p{L}\\\\p{N}]+[\\\\r\\\\n]*|\\\\s*[\\\\r\\\\n]+|\\\\s+(?!\\\\S)|\\\\s+\"\n",
      "            },\n",
      "            \"behavior\": \"Isolated\",\n",
      "            \"invert\": false\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"ByteLevel\",\n",
      "            \"add_prefix_space\": false,\n",
      "            \"trim_offsets\": true,\n",
      "            \"use_regex\": false\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "INFO:convert_hf_to_gguf_update:ignore_merges: false\n",
      "INFO:convert_hf_to_gguf_update:\n",
      "INFO:convert_hf_to_gguf_update:model: poro-chat\n",
      "INFO:convert_hf_to_gguf_update:tokt: 2\n",
      "INFO:convert_hf_to_gguf_update:repo: https://huggingface.co/LumiOpen/Poro-34B-chat\n",
      "INFO:convert_hf_to_gguf_update:chktok: [509, 1353, 27857, 208, 15122, 10405, 51725, 88952, 209, 4318, 259, 233, 365, 16007, 32, 11892, 138, 102753, 117264, 128, 26036, 365, 66533, 2953, 106742, 65851, 708, 32, 38132, 238, 5299, 123, 258, 4318, 123, 258, 639, 10177, 77440, 639, 34205, 639, 11054, 639, 11054, 42, 639, 126803, 639, 11054, 34205, 639, 37, 42, 639, 544, 42, 639, 1290, 42, 231, 19523, 233, 104963, 252, 52087, 244, 19523, 248, 52087, 235, 19523, 255, 19523, 139, 19523, 264, 52087, 234, 19523, 264, 19523, 119, 104963, 238, 45241, 234, 2076, 13217, 37414, 7359, 21264, 55110, 1688, 1581, 45843, 29066, 65074, 263, 69917, 49570, 19777, 12118, 921, 7866, 24106, 24892, 1953, 6197, 13534, 19610, 4055, 90839, 55512, 1928, 84187, 44361, 22174, 3551, 36170, 8711, 5478, 1138, 23492, 1215, 9285, 1152, 35, 468, 1026, 569, 3424, 54, 13125, 611, 3424, 9851, 1522, 435, 35, 18756, 569, 1248, 1191, 21056, 54, 1534, 30, 22798, 285, 49197, 67]\n",
      "INFO:convert_hf_to_gguf_update:chkhsh: c7ea5862a53e4272c035c8238367063e2b270d51faa48c0f09e9d5b54746c360\n",
      "INFO:convert_hf_to_gguf_update:normalizer: null\n",
      "INFO:convert_hf_to_gguf_update:pre_tokenizer: {\n",
      "    \"type\": \"Sequence\",\n",
      "    \"pretokenizers\": [\n",
      "        {\n",
      "            \"type\": \"Split\",\n",
      "            \"pattern\": {\n",
      "                \"Regex\": \" ?[^(\\\\s|[.,!?\\u2026\\u3002\\uff0c\\u3001\\u0964\\u06d4\\u060c])]+\"\n",
      "            },\n",
      "            \"behavior\": \"Isolated\",\n",
      "            \"invert\": false\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"ByteLevel\",\n",
      "            \"add_prefix_space\": false,\n",
      "            \"trim_offsets\": true,\n",
      "            \"use_regex\": false\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "INFO:convert_hf_to_gguf_update:\n",
      "INFO:convert_hf_to_gguf_update:model: jina-v2-code\n",
      "INFO:convert_hf_to_gguf_update:tokt: 2\n",
      "INFO:convert_hf_to_gguf_update:repo: https://huggingface.co/jinaai/jina-embeddings-v2-base-code\n",
      "INFO:convert_hf_to_gguf_update:chktok: [0, 319, 655, 7239, 11489, 274, 6881, 12642, 16716, 203, 8000, 253, 227, 301, 4411, 13, 9919, 251, 119, 2965, 240, 8000, 239, 109, 26726, 301, 10186, 3520, 23869, 302, 45604, 13, 12284, 255, 232, 9919, 104, 252, 8000, 104, 252, 795, 8104, 38292, 795, 9581, 795, 3303, 795, 20428, 795, 13652, 795, 3303, 9581, 795, 18, 23, 795, 419, 23, 795, 1713, 23, 225, 13065, 227, 50218, 13065, 246, 25763, 238, 13065, 242, 25763, 229, 13065, 249, 13065, 120, 13065, 258, 25763, 228, 13065, 258, 13065, 100, 50218, 13065, 232, 8000, 251, 228, 959, 10133, 23692, 5928, 9173, 33543, 1330, 1254, 13567, 22873, 44634, 257, 36031, 12434, 16706, 13633, 1769, 14501, 54827, 21893, 3849, 10107, 13878, 41078, 7095, 9107, 30834, 2678, 1246, 1246, 40651, 13911, 5366, 23681, 7887, 527, 10105, 3081, 363, 88, 1505, 1063, 1476, 2866, 16, 363, 495, 1212, 4509, 35, 363, 49, 691, 4509, 527, 9104, 2554, 605, 16, 363, 40, 1212, 4156, 2681, 4594, 69, 35, 2893, 11, 30247, 323, 11, 80, 48, 2]\n",
      "INFO:convert_hf_to_gguf_update:chkhsh: 7967bfa498ade6b757b064f31e964dddbb80f8f9a4d68d4ba7998fcf281c531a\n",
      "INFO:convert_hf_to_gguf_update:normalizer: null\n",
      "INFO:convert_hf_to_gguf_update:pre_tokenizer: {\n",
      "    \"type\": \"ByteLevel\",\n",
      "    \"add_prefix_space\": false,\n",
      "    \"trim_offsets\": true,\n",
      "    \"use_regex\": true\n",
      "}\n",
      "INFO:convert_hf_to_gguf_update:\n",
      "INFO:convert_hf_to_gguf_update:model: viking\n",
      "INFO:convert_hf_to_gguf_update:tokt: 2\n",
      "INFO:convert_hf_to_gguf_update:repo: https://huggingface.co/LumiOpen/Viking-7B\n",
      "INFO:convert_hf_to_gguf_update:chktok: [746, 2392, 55899, 208, 29480, 19883, 103830, 442, 954, 209, 2781, 259, 233, 376, 24841, 32, 8819, 138, 68448, 63329, 128, 15736, 376, 57353, 1603, 7704, 294, 31756, 4787, 1071, 32, 94674, 3666, 123, 258, 2781, 123, 258, 231, 42, 231, 42, 42, 231, 42, 42, 42, 231, 42, 42, 42, 42, 231, 42, 42, 42, 42, 42, 231, 42, 42, 42, 42, 42, 42, 231, 42, 42, 42, 42, 42, 42, 42, 231, 42, 42, 42, 42, 42, 42, 42, 42, 231, 42, 37, 42, 231, 42, 478, 42, 231, 42, 919, 42, 231, 33532, 233, 33532, 138, 33532, 252, 89464, 244, 33532, 248, 89464, 235, 33532, 255, 33532, 139, 33532, 264, 89464, 234, 33532, 264, 33532, 119, 33532, 138, 33532, 238, 27310, 234, 2748, 23613, 72376, 13227, 42284, 105535, 40, 42, 40, 43, 40, 44, 40, 49056, 43400, 263, 417, 34550, 130761, 28469, 17228, 1085, 13676, 23950, 128516, 56400, 30124, 37322, 108492, 3437, 3395, 3395, 30917, 17846, 2420, 13728, 3963, 9873, 1912, 37493, 733, 17600, 1923, 35, 630, 1417, 791, 6189, 54, 23586, 835, 6189, 18068, 2463, 590, 35, 35018, 791, 1647, 2032, 22940, 54, 2221, 30, 6815, 279, 79905, 67]\n",
      "INFO:convert_hf_to_gguf_update:chkhsh: 7fc505bd3104ca1083b150b17d088b59534ede9bde81f0dd2090967d7fe52cee\n",
      "INFO:convert_hf_to_gguf_update:normalizer: null\n",
      "INFO:convert_hf_to_gguf_update:pre_tokenizer: {\n",
      "    \"type\": \"Sequence\",\n",
      "    \"pretokenizers\": [\n",
      "        {\n",
      "            \"type\": \"Split\",\n",
      "            \"pattern\": {\n",
      "                \"Regex\": \" ?[^(\\\\s|[.,!?\\u2026\\u3002\\uff0c\\u3001\\u0964\\u06d4\\u060c])]+\"\n",
      "            },\n",
      "            \"behavior\": \"Isolated\",\n",
      "            \"invert\": false\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"Digits\",\n",
      "            \"individual_digits\": true\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"ByteLevel\",\n",
      "            \"add_prefix_space\": false,\n",
      "            \"trim_offsets\": true,\n",
      "            \"use_regex\": false\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "INFO:convert_hf_to_gguf_update:\n",
      "INFO:convert_hf_to_gguf_update:model: jais\n",
      "INFO:convert_hf_to_gguf_update:tokt: 2\n",
      "INFO:convert_hf_to_gguf_update:repo: https://huggingface.co/core42/jais-13b\n",
      "INFO:convert_hf_to_gguf_update:chktok: [976, 765, 2680, 976, 185, 208, 1072, 74561, 3463, 870, 3487, 1940, 186, 34469, 236, 210, 414, 12668, 9, 48675, 234, 115, 482, 223, 34469, 222, 105, 170, 117, 225, 414, 75299, 1531, 26289, 351, 71346, 718, 9, 6974, 238, 215, 48675, 100, 235, 34469, 100, 235, 645, 7844, 52640, 645, 36176, 52640, 2807, 52640, 36176, 52640, 57347, 52640, 2807, 36176, 645, 14, 19, 645, 489, 19, 645, 992, 19, 10844, 240, 210, 38572, 115, 38572, 229, 84877, 221, 38572, 225, 84877, 212, 38572, 232, 38572, 116, 38572, 241, 84877, 211, 38572, 241, 38572, 96, 38572, 115, 38572, 215, 34469, 234, 211, 1295, 29970, 79212, 28076, 41773, 34277, 99, 46878, 1612, 1643, 34637, 83965, 43601, 240, 34160, 14140, 27748, 25376, 2083, 18797, 1774, 227, 29314, 7055, 21211, 28183, 22602, 2732, 1127, 12081, 73556, 11024, 11024, 24097, 12864, 12864, 22418, 15989, 2833, 14932, 4808, 450, 3565, 1097, 1127, 84, 1288, 518, 672, 1065, 12, 1127, 3432, 568, 4161, 31, 1127, 45, 649, 4161, 450, 3529, 1952, 540, 12, 1127, 36, 568, 1340, 1209, 20798, 31, 1523, 7, 65885, 321, 7, 76, 44]\n",
      "INFO:convert_hf_to_gguf_update:chkhsh: b53802fb28e26d645c3a310b34bfe07da813026ec7c7716883404d5e0f8b1901\n",
      "INFO:convert_hf_to_gguf_update:normalizer: null\n",
      "INFO:convert_hf_to_gguf_update:pre_tokenizer: {\n",
      "    \"type\": \"ByteLevel\",\n",
      "    \"add_prefix_space\": false,\n",
      "    \"trim_offsets\": true,\n",
      "    \"use_regex\": true\n",
      "}\n",
      "INFO:convert_hf_to_gguf_update:\n",
      "INFO:convert_hf_to_gguf_update:model: codeshell\n",
      "INFO:convert_hf_to_gguf_update:tokt: 2\n",
      "INFO:convert_hf_to_gguf_update:repo: https://huggingface.co/WisdomShell/CodeShell-7B\n",
      "INFO:convert_hf_to_gguf_update:chktok: [2285, 16341, 60076, 10194, 22723, 18379, 13893, 67963, 38270, 54803, 41941, 23719, 40168, 22191, 22033, 46950, 20885, 16201, 41941, 5287, 23665, 46713, 43954, 63990, 19441, 52971, 40168, 40704, 62746, 56896, 17685, 29314, 36365, 58360, 56568, 58360, 56568, 56568, 58360, 56568, 56568, 56568, 58360, 56568, 56568, 56568, 56568, 58360, 56568, 56568, 56568, 56568, 56568, 58360, 56568, 56568, 56568, 56568, 56568, 56568, 58360, 56568, 56568, 56568, 56568, 56568, 56568, 56568, 58360, 56568, 56568, 56568, 56568, 56568, 56568, 56568, 56568, 58360, 56568, 61178, 56568, 58360, 56568, 38216, 56568, 58360, 56568, 44377, 56568, 58360, 62802, 39260, 7787, 40923, 22308, 67724, 16162, 66136, 50933, 66943, 50933, 49541, 39260, 14577, 44699, 35273, 67902, 24364, 37703, 60578, 21533, 56568, 21533, 26384, 21533, 10702, 21533, 10843, 64762, 26351, 64004, 63182, 32115, 58036, 67552, 63195, 15575, 66647, 46293, 25629, 52030, 42150, 17440, 39030, 847, 6303, 62336, 62336, 44694, 38216, 52517, 54599, 10328, 10328, 10328, 23312, 67779, 9461, 68372, 3264, 38697, 20987, 1474, 34423, 37614, 68372, 13425, 25713, 42478, 46275, 68372, 23306, 24917, 42478, 23312, 69469, 28826, 36585, 37614, 68372, 67833, 25713, 53602, 29125, 10441, 48948, 46275, 5208, 67737, 58932, 51664, 67737, 64169, 37422]\n",
      "INFO:convert_hf_to_gguf_update:chkhsh: 7b3e7548e4308f52a76e8229e4e6cc831195d0d1df43aed21ac6c93da05fec5f\n",
      "INFO:convert_hf_to_gguf_update:normalizer: null\n",
      "INFO:convert_hf_to_gguf_update:pre_tokenizer: {\n",
      "    \"type\": \"Sequence\",\n",
      "    \"pretokenizers\": [\n",
      "        {\n",
      "            \"type\": \"Digits\",\n",
      "            \"individual_digits\": true\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"ByteLevel\",\n",
      "            \"add_prefix_space\": false,\n",
      "            \"trim_offsets\": true,\n",
      "            \"use_regex\": true\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "INFO:convert_hf_to_gguf_update:\n",
      "ERROR:convert_hf_to_gguf_update:Error loading tokenizer for model tekken. The model may not exist or is not accessible with the provided token. Error: models/tokenizers/tekken does not appear to have a file named config.json. Checkout 'https://huggingface.co/models/tokenizers/tekken/tree/None' for available files.\n",
      "INFO:convert_hf_to_gguf_update:model: smollm\n",
      "INFO:convert_hf_to_gguf_update:tokt: 2\n",
      "INFO:convert_hf_to_gguf_update:repo: https://huggingface.co/HuggingFaceTB/SmolLM-135M\n",
      "INFO:convert_hf_to_gguf_update:chktok: [3805, 8866, 1116, 3805, 197, 216, 1656, 216, 197, 11181, 472, 2367, 3914, 198, 10813, 244, 218, 365, 5472, 25, 40303, 131, 321, 231, 10813, 230, 121, 31752, 365, 30404, 649, 21658, 271, 46336, 483, 25, 4636, 246, 223, 15107, 116, 243, 10813, 116, 243, 216, 35, 216, 35, 35, 216, 35, 35, 35, 216, 35, 35, 35, 35, 216, 35, 35, 35, 35, 35, 216, 35, 35, 35, 35, 35, 35, 216, 35, 35, 35, 35, 35, 35, 35, 216, 35, 35, 35, 35, 35, 35, 35, 35, 216, 35, 30, 35, 216, 35, 950, 35, 216, 35, 2026, 35, 15822, 248, 218, 40478, 131, 40478, 237, 172, 249, 229, 40478, 233, 172, 249, 220, 40478, 240, 40478, 132, 40478, 249, 172, 249, 219, 40478, 249, 40478, 112, 40478, 131, 40478, 223, 10813, 242, 219, 9148, 19805, 235, 177, 221, 128, 32632, 21949, 36149, 115, 40994, 33, 35, 33, 36, 33, 37, 33, 18614, 119, 186, 138, 248, 216, 21771, 2031, 28733, 28050, 6643, 46438, 6485, 40610, 5470, 235, 156, 228, 12681, 29441, 6511, 9175, 39511, 7872, 7855, 11193, 1969, 1969, 3725, 1093, 1093, 5592, 950, 36689, 10095, 16693, 16693, 16693, 339, 3543, 719, 637, 100, 793, 384, 506, 665, 28, 637, 3256, 346, 2090, 47, 637, 61, 441, 2090, 339, 3060, 919, 357, 28, 637, 52, 346, 702, 634, 7188, 47, 1046, 23, 25917, 253, 23, 92, 60]\n",
      "INFO:convert_hf_to_gguf_update:chkhsh: 855059429035d75a914d1eda9f10a876752e281a054a7a3d421ef0533e5b6249\n",
      "INFO:convert_hf_to_gguf_update:normalizer: null\n",
      "INFO:convert_hf_to_gguf_update:pre_tokenizer: {\n",
      "    \"type\": \"Sequence\",\n",
      "    \"pretokenizers\": [\n",
      "        {\n",
      "            \"type\": \"Digits\",\n",
      "            \"individual_digits\": true\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"ByteLevel\",\n",
      "            \"add_prefix_space\": false,\n",
      "            \"trim_offsets\": true,\n",
      "            \"use_regex\": true\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "INFO:convert_hf_to_gguf_update:ignore_merges: false\n",
      "INFO:convert_hf_to_gguf_update:\n",
      "INFO:convert_hf_to_gguf_update:model: bloom\n",
      "INFO:convert_hf_to_gguf_update:tokt: 2\n",
      "INFO:convert_hf_to_gguf_update:repo: https://huggingface.co/bigscience/bloom\n",
      "INFO:convert_hf_to_gguf_update:chktok: [1782, 6308, 141746, 188, 86863, 33651, 5950, 747, 1699, 2986, 189, 22618, 238, 212, 375, 43364, 12, 126342, 118, 1553, 22618, 224, 108, 116057, 375, 207152, 766, 27548, 290, 102161, 1790, 12, 76758, 217, 41234, 103, 237, 22618, 103, 237, 735, 10491, 100965, 735, 63180, 735, 42456, 735, 42456, 22, 735, 42456, 3960, 735, 42456, 63180, 735, 17, 22, 735, 566, 22, 735, 1369, 22, 98143, 212, 222719, 158304, 168634, 203476, 11208, 234, 203440, 176537, 212727, 176537, 229562, 74907, 217, 127322, 213, 2040, 17765, 648, 78151, 3154, 2482, 2394, 22259, 2129, 30494, 217266, 3654, 53048, 40198, 28270, 2228, 25829, 48405, 37637, 4486, 12099, 37545, 59009, 33334, 6637, 56046, 56046, 56046, 67, 219879, 28757, 75164, 12480, 67870, 17499, 15527, 3784, 40753, 3320, 36773, 2782, 15, 756, 4144, 1152, 11097, 34, 33638, 1130, 11097, 26402, 5219, 718, 15, 54702, 1152, 3269, 3331, 77987, 34, 5361, 10, 56850, 267, 29347, 47]\n",
      "INFO:convert_hf_to_gguf_update:chkhsh: 3c30d3ad1d6b64202cd222813e7736c2db6e1bd6d67197090fc1211fbc612ae7\n",
      "INFO:convert_hf_to_gguf_update:normalizer: null\n",
      "INFO:convert_hf_to_gguf_update:pre_tokenizer: {\n",
      "    \"type\": \"Sequence\",\n",
      "    \"pretokenizers\": [\n",
      "        {\n",
      "            \"type\": \"Split\",\n",
      "            \"pattern\": {\n",
      "                \"Regex\": \" ?[^(\\\\s|[.,!?\\u2026\\u3002\\uff0c\\u3001\\u0964\\u06d4\\u060c])]+\"\n",
      "            },\n",
      "            \"behavior\": \"Isolated\",\n",
      "            \"invert\": false\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"ByteLevel\",\n",
      "            \"add_prefix_space\": false,\n",
      "            \"trim_offsets\": true,\n",
      "            \"use_regex\": false\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "INFO:convert_hf_to_gguf_update:\n",
      "INFO:convert_hf_to_gguf_update:model: gpt3-finnish\n",
      "INFO:convert_hf_to_gguf_update:tokt: 2\n",
      "INFO:convert_hf_to_gguf_update:repo: https://huggingface.co/TurkuNLP/gpt3-finnish-small\n",
      "INFO:convert_hf_to_gguf_update:chktok: [26641, 439, 179, 439, 26641, 176, 179, 176, 176, 179, 176, 26641, 179, 26641, 179, 179, 26641, 179, 179, 179, 26641, 179, 179, 179, 86324, 3603, 206, 180, 444, 12623, 7453, 12, 7788, 118, 338, 193, 3603, 192, 108, 29644, 444, 10857, 237, 32407, 12712, 77599, 12345, 40098, 2622, 6725, 12, 5083, 208, 185, 4061, 103, 205, 3603, 103, 205, 673, 8514, 117996, 8514, 9439, 8514, 9439, 22, 8514, 9439, 9439, 8514, 9439, 9439, 22, 8514, 9439, 9439, 9439, 673, 17, 22, 673, 501, 22, 673, 947, 22, 179, 159, 210, 180, 159, 210, 118, 159, 210, 199, 159, 211, 191, 159, 210, 195, 159, 211, 182, 159, 210, 202, 159, 210, 119, 159, 210, 211, 159, 211, 181, 159, 210, 211, 159, 210, 99, 159, 210, 118, 159, 210, 185, 30541, 181, 6946, 164, 188, 197, 164, 183, 115, 163, 208, 105, 94854, 421, 163, 119, 102, 162, 125, 208, 3453, 3431, 2762, 20, 163, 101, 106, 173, 125, 210, 4113, 46093, 62288, 62288, 62288, 32, 46932, 125, 62077, 143, 189, 54837, 46932, 125, 53294, 46932, 197, 143, 190, 107900, 142, 115, 53294, 74414, 83644, 104452, 65801, 5031, 24795, 24795, 10, 67, 67, 67, 67, 67, 67, 67, 35036, 35036, 42768, 24003, 2545, 48656, 4458, 123961, 35804, 5031, 272, 8891, 660, 7745, 30165, 15, 5031, 19576, 8773, 51535, 34, 5031, 48, 10112, 51535, 38618, 221, 11502, 981, 15, 5031, 39, 8773, 22905, 15023, 72869, 34, 12615, 10, 8524, 249, 10, 79, 47]\n",
      "INFO:convert_hf_to_gguf_update:chkhsh: bc01ce58980e1db43859146dc51b1758b3b88729b217a74792e9f8d43e479d21\n",
      "INFO:convert_hf_to_gguf_update:normalizer: null\n",
      "INFO:convert_hf_to_gguf_update:pre_tokenizer: {\n",
      "    \"type\": \"Sequence\",\n",
      "    \"pretokenizers\": [\n",
      "        {\n",
      "            \"type\": \"Split\",\n",
      "            \"pattern\": {\n",
      "                \"Regex\": \" ?[^(\\\\s|[.,!?\\u2026\\u3002\\uff0c\\u3001\\u0964\\u06d4\\u060c])]+\"\n",
      "            },\n",
      "            \"behavior\": \"Isolated\",\n",
      "            \"invert\": false\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"ByteLevel\",\n",
      "            \"add_prefix_space\": false,\n",
      "            \"trim_offsets\": true,\n",
      "            \"use_regex\": false\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "INFO:convert_hf_to_gguf_update:\n",
      "ERROR:convert_hf_to_gguf_update:Error loading tokenizer for model exaone. The model may not exist or is not accessible with the provided token. Error: models/tokenizers/exaone does not appear to have a file named config.json. Checkout 'https://huggingface.co/models/tokenizers/exaone/tree/None' for available files.\n",
      "Traceback (most recent call last):\n",
      "  File \"/teamspace/studios/this_studio/llama.cpp/convert_hf_to_gguf_update.py\", line 236, in <module>\n",
      "    convert_py = convert_py_pth.read_text(encoding=\"utf-8\")\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/pathlib.py\", line 1134, in read_text\n",
      "    with self.open(mode='r', encoding=encoding, errors=errors) as f:\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/pathlib.py\", line 1119, in open\n",
      "    return self._accessor.open(self, mode, buffering, encoding, errors,\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'convert_hf_to_gguf.py'\n"
     ]
    }
   ],
   "source": [
    "!python llama.cpp/convert_hf_to_gguf_update.py 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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58e8878f-53f7-4140-9811-71291efc9a89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:hf-to-gguf:Loading model: merged_model_final\n",
      "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
      "INFO:hf-to-gguf:Exporting model...\n",
      "INFO:hf-to-gguf:gguf: loading model weight map from 'model.safetensors.index.json'\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00001-of-00002.safetensors'\n",
      "INFO:hf-to-gguf:token_embd.weight,                 torch.float16 --> F16, shape = {2304, 256000}\n",
      "INFO:hf-to-gguf:blk.0.attn_norm.weight,            torch.float16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.0.ffn_down.weight,             torch.float16 --> F16, shape = {9216, 2304}\n",
      "INFO:hf-to-gguf:blk.0.ffn_gate.weight,             torch.float16 --> F16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.0.ffn_up.weight,               torch.float16 --> F16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.0.post_attention_norm.weight,  torch.float16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.0.post_ffw_norm.weight,        torch.float16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.0.ffn_norm.weight,             torch.float16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.0.attn_k.weight,               torch.float16 --> F16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.0.attn_output.weight,          torch.float16 --> F16, shape = {2048, 2304}\n",
      "INFO:hf-to-gguf:blk.0.attn_q.weight,               torch.float16 --> F16, shape = {2304, 2048}\n",
      "INFO:hf-to-gguf:blk.0.attn_v.weight,               torch.float16 --> F16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.1.attn_norm.weight,            torch.float16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.1.ffn_down.weight,             torch.float16 --> F16, shape = {9216, 2304}\n",
      "INFO:hf-to-gguf:blk.1.ffn_gate.weight,             torch.float16 --> F16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.1.ffn_up.weight,               torch.float16 --> F16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.1.post_attention_norm.weight,  torch.float16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.1.post_ffw_norm.weight,        torch.float16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.1.ffn_norm.weight,             torch.float16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.1.attn_k.weight,               torch.float16 --> F16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.1.attn_output.weight,          torch.float16 --> F16, shape = {2048, 2304}\n",
      "INFO:hf-to-gguf:blk.1.attn_q.weight,               torch.float16 --> F16, shape = {2304, 2048}\n",
      "INFO:hf-to-gguf:blk.1.attn_v.weight,               torch.float16 --> F16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.10.attn_norm.weight,           torch.float16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.10.ffn_down.weight,            torch.float16 --> F16, shape = {9216, 2304}\n",
      "INFO:hf-to-gguf:blk.10.ffn_gate.weight,            torch.float16 --> F16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.10.ffn_up.weight,              torch.float16 --> F16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.10.post_attention_norm.weight, torch.float16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.10.post_ffw_norm.weight,       torch.float16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.10.ffn_norm.weight,            torch.float16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.10.attn_k.weight,              torch.float16 --> F16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.10.attn_output.weight,         torch.float16 --> F16, shape = {2048, 2304}\n",
      "INFO:hf-to-gguf:blk.10.attn_q.weight,              torch.float16 --> F16, shape = {2304, 2048}\n",
      "INFO:hf-to-gguf:blk.10.attn_v.weight,              torch.float16 --> F16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.11.attn_norm.weight,           torch.float16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.11.ffn_down.weight,            torch.float16 --> F16, shape = {9216, 2304}\n",
      "INFO:hf-to-gguf:blk.11.ffn_gate.weight,            torch.float16 --> F16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.11.ffn_up.weight,              torch.float16 --> F16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.11.post_attention_norm.weight, torch.float16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.11.post_ffw_norm.weight,       torch.float16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.11.ffn_norm.weight,            torch.float16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.11.attn_k.weight,              torch.float16 --> F16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.11.attn_output.weight,         torch.float16 --> F16, shape = {2048, 2304}\n",
      "INFO:hf-to-gguf:blk.11.attn_q.weight,              torch.float16 --> F16, shape = {2304, 2048}\n",
      "INFO:hf-to-gguf:blk.11.attn_v.weight,              torch.float16 --> F16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.12.attn_norm.weight,           torch.float16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.12.ffn_down.weight,            torch.float16 --> F16, shape = {9216, 2304}\n",
      "INFO:hf-to-gguf:blk.12.ffn_gate.weight,            torch.float16 --> F16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.12.ffn_up.weight,              torch.float16 --> F16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.12.post_attention_norm.weight, torch.float16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.12.post_ffw_norm.weight,       torch.float16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.12.ffn_norm.weight,            torch.float16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.12.attn_k.weight,              torch.float16 --> F16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.12.attn_output.weight,         torch.float16 --> F16, shape = {2048, 2304}\n",
      "INFO:hf-to-gguf:blk.12.attn_q.weight,              torch.float16 --> F16, shape = {2304, 2048}\n",
      "INFO:hf-to-gguf:blk.12.attn_v.weight,              torch.float16 --> F16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.13.attn_norm.weight,           torch.float16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.13.ffn_down.weight,            torch.float16 --> F16, shape = {9216, 2304}\n",
      "INFO:hf-to-gguf:blk.13.ffn_gate.weight,            torch.float16 --> F16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.13.ffn_up.weight,              torch.float16 --> F16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.13.post_attention_norm.weight, torch.float16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.13.post_ffw_norm.weight,       torch.float16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.13.ffn_norm.weight,            torch.float16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.13.attn_k.weight,              torch.float16 --> F16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.13.attn_output.weight,         torch.float16 --> F16, shape = {2048, 2304}\n",
      "INFO:hf-to-gguf:blk.13.attn_q.weight,              torch.float16 --> F16, shape = {2304, 2048}\n",
      "INFO:hf-to-gguf:blk.13.attn_v.weight,              torch.float16 --> F16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.14.attn_norm.weight,           torch.float16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.14.ffn_down.weight,            torch.float16 --> F16, shape = {9216, 2304}\n",
      "INFO:hf-to-gguf:blk.14.ffn_gate.weight,            torch.float16 --> F16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.14.ffn_up.weight,              torch.float16 --> F16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.14.post_attention_norm.weight, torch.float16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.14.post_ffw_norm.weight,       torch.float16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.14.ffn_norm.weight,            torch.float16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.14.attn_k.weight,              torch.float16 --> F16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.14.attn_output.weight,         torch.float16 --> F16, shape = {2048, 2304}\n",
      "INFO:hf-to-gguf:blk.14.attn_q.weight,              torch.float16 --> F16, shape = {2304, 2048}\n",
      "INFO:hf-to-gguf:blk.14.attn_v.weight,              torch.float16 --> F16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.15.attn_norm.weight,           torch.float16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.15.ffn_down.weight,            torch.float16 --> F16, shape = {9216, 2304}\n",
      "INFO:hf-to-gguf:blk.15.ffn_gate.weight,            torch.float16 --> F16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.15.ffn_up.weight,              torch.float16 --> F16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.15.post_attention_norm.weight, torch.float16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.15.post_ffw_norm.weight,       torch.float16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.15.ffn_norm.weight,            torch.float16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.15.attn_k.weight,              torch.float16 --> F16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.15.attn_output.weight,         torch.float16 --> F16, shape = {2048, 2304}\n",
      "INFO:hf-to-gguf:blk.15.attn_q.weight,              torch.float16 --> F16, shape = {2304, 2048}\n",
      "INFO:hf-to-gguf:blk.15.attn_v.weight,              torch.float16 --> F16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.16.attn_norm.weight,           torch.float16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.16.ffn_down.weight,            torch.float16 --> F16, shape = {9216, 2304}\n",
      "INFO:hf-to-gguf:blk.16.ffn_gate.weight,            torch.float16 --> F16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.16.ffn_up.weight,              torch.float16 --> F16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.16.post_attention_norm.weight, torch.float16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.16.post_ffw_norm.weight,       torch.float16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.16.ffn_norm.weight,            torch.float16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.16.attn_k.weight,              torch.float16 --> F16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.16.attn_output.weight,         torch.float16 --> F16, shape = {2048, 2304}\n",
      "INFO:hf-to-gguf:blk.16.attn_q.weight,              torch.float16 --> F16, shape = {2304, 2048}\n",
      "INFO:hf-to-gguf:blk.16.attn_v.weight,              torch.float16 --> F16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.17.attn_norm.weight,           torch.float16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.17.ffn_down.weight,            torch.float16 --> F16, shape = {9216, 2304}\n",
      "INFO:hf-to-gguf:blk.17.ffn_gate.weight,            torch.float16 --> F16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.17.ffn_up.weight,              torch.float16 --> F16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.17.post_attention_norm.weight, torch.float16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.17.post_ffw_norm.weight,       torch.float16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.17.ffn_norm.weight,            torch.float16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.17.attn_k.weight,              torch.float16 --> F16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.17.attn_output.weight,         torch.float16 --> F16, shape = {2048, 2304}\n",
      "INFO:hf-to-gguf:blk.17.attn_q.weight,              torch.float16 --> F16, shape = {2304, 2048}\n",
      "INFO:hf-to-gguf:blk.17.attn_v.weight,              torch.float16 --> F16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.18.attn_norm.weight,           torch.float16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.18.ffn_down.weight,            torch.float16 --> F16, shape = {9216, 2304}\n",
      "INFO:hf-to-gguf:blk.18.ffn_gate.weight,            torch.float16 --> F16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.18.ffn_up.weight,              torch.float16 --> F16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.18.post_attention_norm.weight, torch.float16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.18.post_ffw_norm.weight,       torch.float16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.18.ffn_norm.weight,            torch.float16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.18.attn_k.weight,              torch.float16 --> F16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.18.attn_output.weight,         torch.float16 --> F16, shape = {2048, 2304}\n",
      "INFO:hf-to-gguf:blk.18.attn_q.weight,              torch.float16 --> F16, shape = {2304, 2048}\n",
      "INFO:hf-to-gguf:blk.18.attn_v.weight,              torch.float16 --> F16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.19.attn_norm.weight,           torch.float16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.19.ffn_down.weight,            torch.float16 --> F16, shape = {9216, 2304}\n",
      "INFO:hf-to-gguf:blk.19.ffn_gate.weight,            torch.float16 --> F16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.19.ffn_up.weight,              torch.float16 --> F16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.19.post_attention_norm.weight, torch.float16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.19.post_ffw_norm.weight,       torch.float16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.19.ffn_norm.weight,            torch.float16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.19.attn_k.weight,              torch.float16 --> F16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.19.attn_output.weight,         torch.float16 --> F16, shape = {2048, 2304}\n",
      "INFO:hf-to-gguf:blk.19.attn_q.weight,              torch.float16 --> F16, shape = {2304, 2048}\n",
      "INFO:hf-to-gguf:blk.19.attn_v.weight,              torch.float16 --> F16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.2.attn_norm.weight,            torch.float16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.2.ffn_down.weight,             torch.float16 --> F16, shape = {9216, 2304}\n",
      "INFO:hf-to-gguf:blk.2.ffn_gate.weight,             torch.float16 --> F16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.2.ffn_up.weight,               torch.float16 --> F16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.2.post_attention_norm.weight,  torch.float16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.2.post_ffw_norm.weight,        torch.float16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.2.ffn_norm.weight,             torch.float16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.2.attn_k.weight,               torch.float16 --> F16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.2.attn_output.weight,          torch.float16 --> F16, shape = {2048, 2304}\n",
      "INFO:hf-to-gguf:blk.2.attn_q.weight,               torch.float16 --> F16, shape = {2304, 2048}\n",
      "INFO:hf-to-gguf:blk.2.attn_v.weight,               torch.float16 --> F16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.20.attn_norm.weight,           torch.float16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.20.ffn_down.weight,            torch.float16 --> F16, shape = {9216, 2304}\n",
      "INFO:hf-to-gguf:blk.20.ffn_gate.weight,            torch.float16 --> F16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.20.ffn_up.weight,              torch.float16 --> F16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.20.post_attention_norm.weight, torch.float16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.20.post_ffw_norm.weight,       torch.float16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.20.ffn_norm.weight,            torch.float16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.20.attn_k.weight,              torch.float16 --> F16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.20.attn_output.weight,         torch.float16 --> F16, shape = {2048, 2304}\n",
      "INFO:hf-to-gguf:blk.20.attn_q.weight,              torch.float16 --> F16, shape = {2304, 2048}\n",
      "INFO:hf-to-gguf:blk.20.attn_v.weight,              torch.float16 --> F16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.21.attn_norm.weight,           torch.float16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.21.ffn_down.weight,            torch.float16 --> F16, shape = {9216, 2304}\n",
      "INFO:hf-to-gguf:blk.21.ffn_gate.weight,            torch.float16 --> F16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.21.ffn_up.weight,              torch.float16 --> F16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.21.post_attention_norm.weight, torch.float16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.21.post_ffw_norm.weight,       torch.float16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.21.ffn_norm.weight,            torch.float16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.21.attn_k.weight,              torch.float16 --> F16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.21.attn_output.weight,         torch.float16 --> F16, shape = {2048, 2304}\n",
      "INFO:hf-to-gguf:blk.21.attn_q.weight,              torch.float16 --> F16, shape = {2304, 2048}\n",
      "INFO:hf-to-gguf:blk.21.attn_v.weight,              torch.float16 --> F16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.22.attn_norm.weight,           torch.float16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.22.ffn_down.weight,            torch.float16 --> F16, shape = {9216, 2304}\n",
      "INFO:hf-to-gguf:blk.22.ffn_gate.weight,            torch.float16 --> F16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.22.ffn_up.weight,              torch.float16 --> F16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.22.post_attention_norm.weight, torch.float16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.22.post_ffw_norm.weight,       torch.float16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.22.ffn_norm.weight,            torch.float16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.22.attn_k.weight,              torch.float16 --> F16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.22.attn_output.weight,         torch.float16 --> F16, shape = {2048, 2304}\n",
      "INFO:hf-to-gguf:blk.22.attn_q.weight,              torch.float16 --> F16, shape = {2304, 2048}\n",
      "INFO:hf-to-gguf:blk.22.attn_v.weight,              torch.float16 --> F16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.23.attn_norm.weight,           torch.float16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.23.ffn_down.weight,            torch.float16 --> F16, shape = {9216, 2304}\n",
      "INFO:hf-to-gguf:blk.23.ffn_gate.weight,            torch.float16 --> F16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.23.ffn_up.weight,              torch.float16 --> F16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.23.post_attention_norm.weight, torch.float16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.23.post_ffw_norm.weight,       torch.float16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.23.ffn_norm.weight,            torch.float16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.23.attn_k.weight,              torch.float16 --> F16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.23.attn_output.weight,         torch.float16 --> F16, shape = {2048, 2304}\n",
      "INFO:hf-to-gguf:blk.23.attn_q.weight,              torch.float16 --> F16, shape = {2304, 2048}\n",
      "INFO:hf-to-gguf:blk.23.attn_v.weight,              torch.float16 --> F16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.24.ffn_gate.weight,            torch.float16 --> F16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.24.attn_k.weight,              torch.float16 --> F16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.24.attn_output.weight,         torch.float16 --> F16, shape = {2048, 2304}\n",
      "INFO:hf-to-gguf:blk.24.attn_q.weight,              torch.float16 --> F16, shape = {2304, 2048}\n",
      "INFO:hf-to-gguf:blk.24.attn_v.weight,              torch.float16 --> F16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.3.attn_norm.weight,            torch.float16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.3.ffn_down.weight,             torch.float16 --> F16, shape = {9216, 2304}\n",
      "INFO:hf-to-gguf:blk.3.ffn_gate.weight,             torch.float16 --> F16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.3.ffn_up.weight,               torch.float16 --> F16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.3.post_attention_norm.weight,  torch.float16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.3.post_ffw_norm.weight,        torch.float16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.3.ffn_norm.weight,             torch.float16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.3.attn_k.weight,               torch.float16 --> F16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.3.attn_output.weight,          torch.float16 --> F16, shape = {2048, 2304}\n",
      "INFO:hf-to-gguf:blk.3.attn_q.weight,               torch.float16 --> F16, shape = {2304, 2048}\n",
      "INFO:hf-to-gguf:blk.3.attn_v.weight,               torch.float16 --> F16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.4.attn_norm.weight,            torch.float16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.4.ffn_down.weight,             torch.float16 --> F16, shape = {9216, 2304}\n",
      "INFO:hf-to-gguf:blk.4.ffn_gate.weight,             torch.float16 --> F16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.4.ffn_up.weight,               torch.float16 --> F16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.4.post_attention_norm.weight,  torch.float16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.4.post_ffw_norm.weight,        torch.float16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.4.ffn_norm.weight,             torch.float16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.4.attn_k.weight,               torch.float16 --> F16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.4.attn_output.weight,          torch.float16 --> F16, shape = {2048, 2304}\n",
      "INFO:hf-to-gguf:blk.4.attn_q.weight,               torch.float16 --> F16, shape = {2304, 2048}\n",
      "INFO:hf-to-gguf:blk.4.attn_v.weight,               torch.float16 --> F16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.5.attn_norm.weight,            torch.float16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.5.ffn_down.weight,             torch.float16 --> F16, shape = {9216, 2304}\n",
      "INFO:hf-to-gguf:blk.5.ffn_gate.weight,             torch.float16 --> F16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.5.ffn_up.weight,               torch.float16 --> F16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.5.post_attention_norm.weight,  torch.float16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.5.post_ffw_norm.weight,        torch.float16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.5.ffn_norm.weight,             torch.float16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.5.attn_k.weight,               torch.float16 --> F16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.5.attn_output.weight,          torch.float16 --> F16, shape = {2048, 2304}\n",
      "INFO:hf-to-gguf:blk.5.attn_q.weight,               torch.float16 --> F16, shape = {2304, 2048}\n",
      "INFO:hf-to-gguf:blk.5.attn_v.weight,               torch.float16 --> F16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.6.attn_norm.weight,            torch.float16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.6.ffn_down.weight,             torch.float16 --> F16, shape = {9216, 2304}\n",
      "INFO:hf-to-gguf:blk.6.ffn_gate.weight,             torch.float16 --> F16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.6.ffn_up.weight,               torch.float16 --> F16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.6.post_attention_norm.weight,  torch.float16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.6.post_ffw_norm.weight,        torch.float16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.6.ffn_norm.weight,             torch.float16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.6.attn_k.weight,               torch.float16 --> F16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.6.attn_output.weight,          torch.float16 --> F16, shape = {2048, 2304}\n",
      "INFO:hf-to-gguf:blk.6.attn_q.weight,               torch.float16 --> F16, shape = {2304, 2048}\n",
      "INFO:hf-to-gguf:blk.6.attn_v.weight,               torch.float16 --> F16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.7.attn_norm.weight,            torch.float16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.7.ffn_down.weight,             torch.float16 --> F16, shape = {9216, 2304}\n",
      "INFO:hf-to-gguf:blk.7.ffn_gate.weight,             torch.float16 --> F16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.7.ffn_up.weight,               torch.float16 --> F16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.7.post_attention_norm.weight,  torch.float16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.7.post_ffw_norm.weight,        torch.float16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.7.ffn_norm.weight,             torch.float16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.7.attn_k.weight,               torch.float16 --> F16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.7.attn_output.weight,          torch.float16 --> F16, shape = {2048, 2304}\n",
      "INFO:hf-to-gguf:blk.7.attn_q.weight,               torch.float16 --> F16, shape = {2304, 2048}\n",
      "INFO:hf-to-gguf:blk.7.attn_v.weight,               torch.float16 --> F16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.8.attn_norm.weight,            torch.float16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.8.ffn_down.weight,             torch.float16 --> F16, shape = {9216, 2304}\n",
      "INFO:hf-to-gguf:blk.8.ffn_gate.weight,             torch.float16 --> F16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.8.ffn_up.weight,               torch.float16 --> F16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.8.post_attention_norm.weight,  torch.float16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.8.post_ffw_norm.weight,        torch.float16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.8.ffn_norm.weight,             torch.float16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.8.attn_k.weight,               torch.float16 --> F16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.8.attn_output.weight,          torch.float16 --> F16, shape = {2048, 2304}\n",
      "INFO:hf-to-gguf:blk.8.attn_q.weight,               torch.float16 --> F16, shape = {2304, 2048}\n",
      "INFO:hf-to-gguf:blk.8.attn_v.weight,               torch.float16 --> F16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.9.attn_norm.weight,            torch.float16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.9.ffn_down.weight,             torch.float16 --> F16, shape = {9216, 2304}\n",
      "INFO:hf-to-gguf:blk.9.ffn_gate.weight,             torch.float16 --> F16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.9.ffn_up.weight,               torch.float16 --> F16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.9.post_attention_norm.weight,  torch.float16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.9.post_ffw_norm.weight,        torch.float16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.9.ffn_norm.weight,             torch.float16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.9.attn_k.weight,               torch.float16 --> F16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.9.attn_output.weight,          torch.float16 --> F16, shape = {2048, 2304}\n",
      "INFO:hf-to-gguf:blk.9.attn_q.weight,               torch.float16 --> F16, shape = {2304, 2048}\n",
      "INFO:hf-to-gguf:blk.9.attn_v.weight,               torch.float16 --> F16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00002-of-00002.safetensors'\n",
      "INFO:hf-to-gguf:blk.24.attn_norm.weight,           torch.float16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.24.ffn_down.weight,            torch.float16 --> F16, shape = {9216, 2304}\n",
      "INFO:hf-to-gguf:blk.24.ffn_up.weight,              torch.float16 --> F16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.24.post_attention_norm.weight, torch.float16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.24.post_ffw_norm.weight,       torch.float16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.24.ffn_norm.weight,            torch.float16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.25.attn_norm.weight,           torch.float16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.25.ffn_down.weight,            torch.float16 --> F16, shape = {9216, 2304}\n",
      "INFO:hf-to-gguf:blk.25.ffn_gate.weight,            torch.float16 --> F16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.25.ffn_up.weight,              torch.float16 --> F16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.25.post_attention_norm.weight, torch.float16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.25.post_ffw_norm.weight,       torch.float16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.25.ffn_norm.weight,            torch.float16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.25.attn_k.weight,              torch.float16 --> F16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.25.attn_output.weight,         torch.float16 --> F16, shape = {2048, 2304}\n",
      "INFO:hf-to-gguf:blk.25.attn_q.weight,              torch.float16 --> F16, shape = {2304, 2048}\n",
      "INFO:hf-to-gguf:blk.25.attn_v.weight,              torch.float16 --> F16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:output_norm.weight,                torch.float16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:Set meta model\n",
      "INFO:hf-to-gguf:Set model parameters\n",
      "INFO:hf-to-gguf:Set model tokenizer\n",
      "INFO:gguf.vocab:Setting special token type bos to 2\n",
      "INFO:gguf.vocab:Setting special token type eos to 1\n",
      "INFO:gguf.vocab:Setting special token type unk to 3\n",
      "INFO:gguf.vocab:Setting special token type pad to 0\n",
      "INFO:gguf.vocab:Setting add_bos_token to True\n",
      "INFO:gguf.vocab:Setting add_eos_token to False\n",
      "INFO:hf-to-gguf:Set model quantization version\n",
      "INFO:gguf.gguf_writer:Writing the following files:\n",
      "INFO:gguf.gguf_writer:quantized_model_final/FP16.gguf: n_tensors = 288, total_size = 5.2G\n",
      "Writing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5.23G/5.23G [00:29<00:00, 176Mbyte/s]\n",
      "INFO:hf-to-gguf:Model successfully exported to quantized_model_final/FP16.gguf\n"
     ]
    }
   ],
   "source": [
    "!python llama.cpp/convert_hf_to_gguf.py ./merged_model_final/ --outtype f16 --outfile ./quantized_model_final/FP16.gguf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "044d44a8-683d-45b9-8013-10fc5e0d843e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Makefile:76: *** LLAMA_CUBLAS is removed. Use GGML_CUDA instead..  Stop.\n"
     ]
    }
   ],
   "source": [
    "# !cd llama.cpp && LLAMA_CUBLAS=1 make && pip install -r requirements/requirements-convert-hf-to-gguf.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21acf0c3-2f2e-4cff-b217-c567138ae4b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 3605 (cfac111e)\n",
      "main: built with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\n",
      "main: quantizing './quantized_model_final/FP16.gguf' to './quantized_model_final/ft_Q4_K_M.gguf' as Q4_K_M\n",
      "llama_model_loader: loaded meta data with 33 key-value pairs and 288 tensors from ./quantized_model_final/FP16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Gemma 2 2b It\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Google\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = it\n",
      "llama_model_loader: - kv   5:                           general.basename str              = gemma-2\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 2B\n",
      "llama_model_loader: - kv   7:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   8:                    gemma2.embedding_length u32              = 2304\n",
      "llama_model_loader: - kv   9:                         gemma2.block_count u32              = 26\n",
      "llama_model_loader: - kv  10:                 gemma2.feed_forward_length u32              = 9216\n",
      "llama_model_loader: - kv  11:                gemma2.attention.head_count u32              = 8\n",
      "llama_model_loader: - kv  12:             gemma2.attention.head_count_kv u32              = 4\n",
      "llama_model_loader: - kv  13:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  14:                gemma2.attention.key_length u32              = 256\n",
      "llama_model_loader: - kv  15:              gemma2.attention.value_length u32              = 256\n",
      "llama_model_loader: - kv  16:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  17:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  18:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  19:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  23:                      tokenizer.ggml.scores arr[f32,256000]  = [-1000.000000, -1000.000000, -1000.00...\n",
      "llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  28:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  29:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  30:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  31:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  32:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:  105 tensors\n",
      "llama_model_loader: - type  f16:  183 tensors\n",
      "[   1/ 288]                    token_embd.weight - [ 2304, 256000,     1,     1], type =    f16, converting to q6_K .. size =  1125.00 MiB ->   461.43 MiB\n",
      "[   2/ 288]               blk.0.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[   3/ 288]                blk.0.ffn_down.weight - [ 9216,  2304,     1,     1], type =    f16, converting to q6_K .. size =    40.50 MiB ->    16.61 MiB\n",
      "[   4/ 288]                blk.0.ffn_gate.weight - [ 2304,  9216,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[   5/ 288]                  blk.0.ffn_up.weight - [ 2304,  9216,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[   6/ 288]     blk.0.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[   7/ 288]           blk.0.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[   8/ 288]                blk.0.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[   9/ 288]                  blk.0.attn_k.weight - [ 2304,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[  10/ 288]             blk.0.attn_output.weight - [ 2048,  2304,     1,     1], type =    f16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[  11/ 288]                  blk.0.attn_q.weight - [ 2304,  2048,     1,     1], type =    f16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[  12/ 288]                  blk.0.attn_v.weight - [ 2304,  1024,     1,     1], type =    f16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n",
      "[  13/ 288]               blk.1.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  14/ 288]                blk.1.ffn_down.weight - [ 9216,  2304,     1,     1], type =    f16, converting to q6_K .. size =    40.50 MiB ->    16.61 MiB\n",
      "[  15/ 288]                blk.1.ffn_gate.weight - [ 2304,  9216,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[  16/ 288]                  blk.1.ffn_up.weight - [ 2304,  9216,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[  17/ 288]     blk.1.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  18/ 288]           blk.1.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  19/ 288]                blk.1.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  20/ 288]                  blk.1.attn_k.weight - [ 2304,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[  21/ 288]             blk.1.attn_output.weight - [ 2048,  2304,     1,     1], type =    f16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[  22/ 288]                  blk.1.attn_q.weight - [ 2304,  2048,     1,     1], type =    f16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[  23/ 288]                  blk.1.attn_v.weight - [ 2304,  1024,     1,     1], type =    f16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n",
      "[  24/ 288]              blk.10.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  25/ 288]               blk.10.ffn_down.weight - [ 9216,  2304,     1,     1], type =    f16, converting to q6_K .. size =    40.50 MiB ->    16.61 MiB\n",
      "[  26/ 288]               blk.10.ffn_gate.weight - [ 2304,  9216,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[  27/ 288]                 blk.10.ffn_up.weight - [ 2304,  9216,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[  28/ 288]    blk.10.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  29/ 288]          blk.10.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  30/ 288]               blk.10.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  31/ 288]                 blk.10.attn_k.weight - [ 2304,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[  32/ 288]            blk.10.attn_output.weight - [ 2048,  2304,     1,     1], type =    f16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[  33/ 288]                 blk.10.attn_q.weight - [ 2304,  2048,     1,     1], type =    f16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[  34/ 288]                 blk.10.attn_v.weight - [ 2304,  1024,     1,     1], type =    f16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n",
      "[  35/ 288]              blk.11.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  36/ 288]               blk.11.ffn_down.weight - [ 9216,  2304,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[  37/ 288]               blk.11.ffn_gate.weight - [ 2304,  9216,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[  38/ 288]                 blk.11.ffn_up.weight - [ 2304,  9216,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[  39/ 288]    blk.11.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  40/ 288]          blk.11.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  41/ 288]               blk.11.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  42/ 288]                 blk.11.attn_k.weight - [ 2304,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[  43/ 288]            blk.11.attn_output.weight - [ 2048,  2304,     1,     1], type =    f16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[  44/ 288]                 blk.11.attn_q.weight - [ 2304,  2048,     1,     1], type =    f16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[  45/ 288]                 blk.11.attn_v.weight - [ 2304,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[  46/ 288]              blk.12.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  47/ 288]               blk.12.ffn_down.weight - [ 9216,  2304,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[  48/ 288]               blk.12.ffn_gate.weight - [ 2304,  9216,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[  49/ 288]                 blk.12.ffn_up.weight - [ 2304,  9216,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[  50/ 288]    blk.12.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  51/ 288]          blk.12.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  52/ 288]               blk.12.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  53/ 288]                 blk.12.attn_k.weight - [ 2304,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[  54/ 288]            blk.12.attn_output.weight - [ 2048,  2304,     1,     1], type =    f16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[  55/ 288]                 blk.12.attn_q.weight - [ 2304,  2048,     1,     1], type =    f16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[  56/ 288]                 blk.12.attn_v.weight - [ 2304,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[  57/ 288]              blk.13.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  58/ 288]               blk.13.ffn_down.weight - [ 9216,  2304,     1,     1], type =    f16, converting to q6_K .. size =    40.50 MiB ->    16.61 MiB\n",
      "[  59/ 288]               blk.13.ffn_gate.weight - [ 2304,  9216,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[  60/ 288]                 blk.13.ffn_up.weight - [ 2304,  9216,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[  61/ 288]    blk.13.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  62/ 288]          blk.13.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  63/ 288]               blk.13.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  64/ 288]                 blk.13.attn_k.weight - [ 2304,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[  65/ 288]            blk.13.attn_output.weight - [ 2048,  2304,     1,     1], type =    f16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[  66/ 288]                 blk.13.attn_q.weight - [ 2304,  2048,     1,     1], type =    f16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[  67/ 288]                 blk.13.attn_v.weight - [ 2304,  1024,     1,     1], type =    f16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n",
      "[  68/ 288]              blk.14.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  69/ 288]               blk.14.ffn_down.weight - [ 9216,  2304,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[  70/ 288]               blk.14.ffn_gate.weight - [ 2304,  9216,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[  71/ 288]                 blk.14.ffn_up.weight - [ 2304,  9216,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[  72/ 288]    blk.14.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  73/ 288]          blk.14.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  74/ 288]               blk.14.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  75/ 288]                 blk.14.attn_k.weight - [ 2304,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[  76/ 288]            blk.14.attn_output.weight - [ 2048,  2304,     1,     1], type =    f16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[  77/ 288]                 blk.14.attn_q.weight - [ 2304,  2048,     1,     1], type =    f16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[  78/ 288]                 blk.14.attn_v.weight - [ 2304,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[  79/ 288]              blk.15.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  80/ 288]               blk.15.ffn_down.weight - [ 9216,  2304,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[  81/ 288]               blk.15.ffn_gate.weight - [ 2304,  9216,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[  82/ 288]                 blk.15.ffn_up.weight - [ 2304,  9216,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[  83/ 288]    blk.15.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  84/ 288]          blk.15.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  85/ 288]               blk.15.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  86/ 288]                 blk.15.attn_k.weight - [ 2304,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[  87/ 288]            blk.15.attn_output.weight - [ 2048,  2304,     1,     1], type =    f16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[  88/ 288]                 blk.15.attn_q.weight - [ 2304,  2048,     1,     1], type =    f16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[  89/ 288]                 blk.15.attn_v.weight - [ 2304,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[  90/ 288]              blk.16.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  91/ 288]               blk.16.ffn_down.weight - [ 9216,  2304,     1,     1], type =    f16, converting to q6_K .. size =    40.50 MiB ->    16.61 MiB\n",
      "[  92/ 288]               blk.16.ffn_gate.weight - [ 2304,  9216,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[  93/ 288]                 blk.16.ffn_up.weight - [ 2304,  9216,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[  94/ 288]    blk.16.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  95/ 288]          blk.16.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  96/ 288]               blk.16.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  97/ 288]                 blk.16.attn_k.weight - [ 2304,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[  98/ 288]            blk.16.attn_output.weight - [ 2048,  2304,     1,     1], type =    f16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[  99/ 288]                 blk.16.attn_q.weight - [ 2304,  2048,     1,     1], type =    f16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[ 100/ 288]                 blk.16.attn_v.weight - [ 2304,  1024,     1,     1], type =    f16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n",
      "[ 101/ 288]              blk.17.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 102/ 288]               blk.17.ffn_down.weight - [ 9216,  2304,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 103/ 288]               blk.17.ffn_gate.weight - [ 2304,  9216,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 104/ 288]                 blk.17.ffn_up.weight - [ 2304,  9216,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 105/ 288]    blk.17.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 106/ 288]          blk.17.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 107/ 288]               blk.17.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 108/ 288]                 blk.17.attn_k.weight - [ 2304,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[ 109/ 288]            blk.17.attn_output.weight - [ 2048,  2304,     1,     1], type =    f16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[ 110/ 288]                 blk.17.attn_q.weight - [ 2304,  2048,     1,     1], type =    f16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[ 111/ 288]                 blk.17.attn_v.weight - [ 2304,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[ 112/ 288]              blk.18.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 113/ 288]               blk.18.ffn_down.weight - [ 9216,  2304,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 114/ 288]               blk.18.ffn_gate.weight - [ 2304,  9216,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 115/ 288]                 blk.18.ffn_up.weight - [ 2304,  9216,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 116/ 288]    blk.18.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 117/ 288]          blk.18.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 118/ 288]               blk.18.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 119/ 288]                 blk.18.attn_k.weight - [ 2304,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[ 120/ 288]            blk.18.attn_output.weight - [ 2048,  2304,     1,     1], type =    f16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[ 121/ 288]                 blk.18.attn_q.weight - [ 2304,  2048,     1,     1], type =    f16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[ 122/ 288]                 blk.18.attn_v.weight - [ 2304,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[ 123/ 288]              blk.19.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 124/ 288]               blk.19.ffn_down.weight - [ 9216,  2304,     1,     1], type =    f16, converting to q6_K .. size =    40.50 MiB ->    16.61 MiB\n",
      "[ 125/ 288]               blk.19.ffn_gate.weight - [ 2304,  9216,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 126/ 288]                 blk.19.ffn_up.weight - [ 2304,  9216,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 127/ 288]    blk.19.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 128/ 288]          blk.19.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 129/ 288]               blk.19.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 130/ 288]                 blk.19.attn_k.weight - [ 2304,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[ 131/ 288]            blk.19.attn_output.weight - [ 2048,  2304,     1,     1], type =    f16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[ 132/ 288]                 blk.19.attn_q.weight - [ 2304,  2048,     1,     1], type =    f16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[ 133/ 288]                 blk.19.attn_v.weight - [ 2304,  1024,     1,     1], type =    f16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n",
      "[ 134/ 288]               blk.2.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 135/ 288]                blk.2.ffn_down.weight - [ 9216,  2304,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 136/ 288]                blk.2.ffn_gate.weight - [ 2304,  9216,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 137/ 288]                  blk.2.ffn_up.weight - [ 2304,  9216,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 138/ 288]     blk.2.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 139/ 288]           blk.2.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 140/ 288]                blk.2.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 141/ 288]                  blk.2.attn_k.weight - [ 2304,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[ 142/ 288]             blk.2.attn_output.weight - [ 2048,  2304,     1,     1], type =    f16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[ 143/ 288]                  blk.2.attn_q.weight - [ 2304,  2048,     1,     1], type =    f16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[ 144/ 288]                  blk.2.attn_v.weight - [ 2304,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[ 145/ 288]              blk.20.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 146/ 288]               blk.20.ffn_down.weight - [ 9216,  2304,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 147/ 288]               blk.20.ffn_gate.weight - [ 2304,  9216,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 148/ 288]                 blk.20.ffn_up.weight - [ 2304,  9216,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 149/ 288]    blk.20.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 150/ 288]          blk.20.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 151/ 288]               blk.20.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 152/ 288]                 blk.20.attn_k.weight - [ 2304,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[ 153/ 288]            blk.20.attn_output.weight - [ 2048,  2304,     1,     1], type =    f16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[ 154/ 288]                 blk.20.attn_q.weight - [ 2304,  2048,     1,     1], type =    f16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[ 155/ 288]                 blk.20.attn_v.weight - [ 2304,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[ 156/ 288]              blk.21.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 157/ 288]               blk.21.ffn_down.weight - [ 9216,  2304,     1,     1], type =    f16, converting to q6_K .. size =    40.50 MiB ->    16.61 MiB\n",
      "[ 158/ 288]               blk.21.ffn_gate.weight - [ 2304,  9216,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 159/ 288]                 blk.21.ffn_up.weight - [ 2304,  9216,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 160/ 288]    blk.21.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 161/ 288]          blk.21.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 162/ 288]               blk.21.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 163/ 288]                 blk.21.attn_k.weight - [ 2304,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[ 164/ 288]            blk.21.attn_output.weight - [ 2048,  2304,     1,     1], type =    f16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[ 165/ 288]                 blk.21.attn_q.weight - [ 2304,  2048,     1,     1], type =    f16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[ 166/ 288]                 blk.21.attn_v.weight - [ 2304,  1024,     1,     1], type =    f16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n",
      "[ 167/ 288]              blk.22.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 168/ 288]               blk.22.ffn_down.weight - [ 9216,  2304,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 169/ 288]               blk.22.ffn_gate.weight - [ 2304,  9216,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 170/ 288]                 blk.22.ffn_up.weight - [ 2304,  9216,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 171/ 288]    blk.22.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 172/ 288]          blk.22.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 173/ 288]               blk.22.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 174/ 288]                 blk.22.attn_k.weight - [ 2304,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[ 175/ 288]            blk.22.attn_output.weight - [ 2048,  2304,     1,     1], type =    f16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[ 176/ 288]                 blk.22.attn_q.weight - [ 2304,  2048,     1,     1], type =    f16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[ 177/ 288]                 blk.22.attn_v.weight - [ 2304,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[ 178/ 288]              blk.23.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 179/ 288]               blk.23.ffn_down.weight - [ 9216,  2304,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 180/ 288]               blk.23.ffn_gate.weight - [ 2304,  9216,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 181/ 288]                 blk.23.ffn_up.weight - [ 2304,  9216,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 182/ 288]    blk.23.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 183/ 288]          blk.23.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 184/ 288]               blk.23.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 185/ 288]                 blk.23.attn_k.weight - [ 2304,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[ 186/ 288]            blk.23.attn_output.weight - [ 2048,  2304,     1,     1], type =    f16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[ 187/ 288]                 blk.23.attn_q.weight - [ 2304,  2048,     1,     1], type =    f16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[ 188/ 288]                 blk.23.attn_v.weight - [ 2304,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[ 189/ 288]               blk.24.ffn_gate.weight - [ 2304,  9216,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 190/ 288]                 blk.24.attn_k.weight - [ 2304,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[ 191/ 288]            blk.24.attn_output.weight - [ 2048,  2304,     1,     1], type =    f16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[ 192/ 288]                 blk.24.attn_q.weight - [ 2304,  2048,     1,     1], type =    f16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[ 193/ 288]                 blk.24.attn_v.weight - [ 2304,  1024,     1,     1], type =    f16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n",
      "[ 194/ 288]               blk.3.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 195/ 288]                blk.3.ffn_down.weight - [ 9216,  2304,     1,     1], type =    f16, converting to q6_K .. size =    40.50 MiB ->    16.61 MiB\n",
      "[ 196/ 288]                blk.3.ffn_gate.weight - [ 2304,  9216,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 197/ 288]                  blk.3.ffn_up.weight - [ 2304,  9216,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 198/ 288]     blk.3.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 199/ 288]           blk.3.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 200/ 288]                blk.3.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 201/ 288]                  blk.3.attn_k.weight - [ 2304,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[ 202/ 288]             blk.3.attn_output.weight - [ 2048,  2304,     1,     1], type =    f16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[ 203/ 288]                  blk.3.attn_q.weight - [ 2304,  2048,     1,     1], type =    f16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[ 204/ 288]                  blk.3.attn_v.weight - [ 2304,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[ 205/ 288]               blk.4.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 206/ 288]                blk.4.ffn_down.weight - [ 9216,  2304,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 207/ 288]                blk.4.ffn_gate.weight - [ 2304,  9216,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 208/ 288]                  blk.4.ffn_up.weight - [ 2304,  9216,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 209/ 288]     blk.4.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 210/ 288]           blk.4.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 211/ 288]                blk.4.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 212/ 288]                  blk.4.attn_k.weight - [ 2304,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[ 213/ 288]             blk.4.attn_output.weight - [ 2048,  2304,     1,     1], type =    f16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[ 214/ 288]                  blk.4.attn_q.weight - [ 2304,  2048,     1,     1], type =    f16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[ 215/ 288]                  blk.4.attn_v.weight - [ 2304,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[ 216/ 288]               blk.5.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 217/ 288]                blk.5.ffn_down.weight - [ 9216,  2304,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 218/ 288]                blk.5.ffn_gate.weight - [ 2304,  9216,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 219/ 288]                  blk.5.ffn_up.weight - [ 2304,  9216,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 220/ 288]     blk.5.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 221/ 288]           blk.5.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 222/ 288]                blk.5.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 223/ 288]                  blk.5.attn_k.weight - [ 2304,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[ 224/ 288]             blk.5.attn_output.weight - [ 2048,  2304,     1,     1], type =    f16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[ 225/ 288]                  blk.5.attn_q.weight - [ 2304,  2048,     1,     1], type =    f16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[ 226/ 288]                  blk.5.attn_v.weight - [ 2304,  1024,     1,     1], type =    f16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n",
      "[ 227/ 288]               blk.6.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 228/ 288]                blk.6.ffn_down.weight - [ 9216,  2304,     1,     1], type =    f16, converting to q6_K .. size =    40.50 MiB ->    16.61 MiB\n",
      "[ 229/ 288]                blk.6.ffn_gate.weight - [ 2304,  9216,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 230/ 288]                  blk.6.ffn_up.weight - [ 2304,  9216,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 231/ 288]     blk.6.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 232/ 288]           blk.6.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 233/ 288]                blk.6.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 234/ 288]                  blk.6.attn_k.weight - [ 2304,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[ 235/ 288]             blk.6.attn_output.weight - [ 2048,  2304,     1,     1], type =    f16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[ 236/ 288]                  blk.6.attn_q.weight - [ 2304,  2048,     1,     1], type =    f16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[ 237/ 288]                  blk.6.attn_v.weight - [ 2304,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[ 238/ 288]               blk.7.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 239/ 288]                blk.7.ffn_down.weight - [ 9216,  2304,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 240/ 288]                blk.7.ffn_gate.weight - [ 2304,  9216,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 241/ 288]                  blk.7.ffn_up.weight - [ 2304,  9216,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 242/ 288]     blk.7.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 243/ 288]           blk.7.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 244/ 288]                blk.7.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 245/ 288]                  blk.7.attn_k.weight - [ 2304,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[ 246/ 288]             blk.7.attn_output.weight - [ 2048,  2304,     1,     1], type =    f16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[ 247/ 288]                  blk.7.attn_q.weight - [ 2304,  2048,     1,     1], type =    f16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[ 248/ 288]                  blk.7.attn_v.weight - [ 2304,  1024,     1,     1], type =    f16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n",
      "[ 249/ 288]               blk.8.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 250/ 288]                blk.8.ffn_down.weight - [ 9216,  2304,     1,     1], type =    f16, converting to q6_K .. size =    40.50 MiB ->    16.61 MiB\n",
      "[ 251/ 288]                blk.8.ffn_gate.weight - [ 2304,  9216,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 252/ 288]                  blk.8.ffn_up.weight - [ 2304,  9216,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 253/ 288]     blk.8.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 254/ 288]           blk.8.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 255/ 288]                blk.8.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 256/ 288]                  blk.8.attn_k.weight - [ 2304,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[ 257/ 288]             blk.8.attn_output.weight - [ 2048,  2304,     1,     1], type =    f16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[ 258/ 288]                  blk.8.attn_q.weight - [ 2304,  2048,     1,     1], type =    f16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[ 259/ 288]                  blk.8.attn_v.weight - [ 2304,  1024,     1,     1], type =    f16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n",
      "[ 260/ 288]               blk.9.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 261/ 288]                blk.9.ffn_down.weight - [ 9216,  2304,     1,     1], type =    f16, converting to q6_K .. size =    40.50 MiB ->    16.61 MiB\n",
      "[ 262/ 288]                blk.9.ffn_gate.weight - [ 2304,  9216,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 263/ 288]                  blk.9.ffn_up.weight - [ 2304,  9216,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 264/ 288]     blk.9.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 265/ 288]           blk.9.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 266/ 288]                blk.9.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 267/ 288]                  blk.9.attn_k.weight - [ 2304,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[ 268/ 288]             blk.9.attn_output.weight - [ 2048,  2304,     1,     1], type =    f16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[ 269/ 288]                  blk.9.attn_q.weight - [ 2304,  2048,     1,     1], type =    f16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[ 270/ 288]                  blk.9.attn_v.weight - [ 2304,  1024,     1,     1], type =    f16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n",
      "[ 271/ 288]              blk.24.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 272/ 288]               blk.24.ffn_down.weight - [ 9216,  2304,     1,     1], type =    f16, converting to q6_K .. size =    40.50 MiB ->    16.61 MiB\n",
      "[ 273/ 288]                 blk.24.ffn_up.weight - [ 2304,  9216,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 274/ 288]    blk.24.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 275/ 288]          blk.24.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 276/ 288]               blk.24.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 277/ 288]              blk.25.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 278/ 288]               blk.25.ffn_down.weight - [ 9216,  2304,     1,     1], type =    f16, converting to q6_K .. size =    40.50 MiB ->    16.61 MiB\n",
      "[ 279/ 288]               blk.25.ffn_gate.weight - [ 2304,  9216,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 280/ 288]                 blk.25.ffn_up.weight - [ 2304,  9216,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 281/ 288]    blk.25.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 282/ 288]          blk.25.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 283/ 288]               blk.25.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 284/ 288]                 blk.25.attn_k.weight - [ 2304,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[ 285/ 288]            blk.25.attn_output.weight - [ 2048,  2304,     1,     1], type =    f16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[ 286/ 288]                 blk.25.attn_q.weight - [ 2304,  2048,     1,     1], type =    f16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[ 287/ 288]                 blk.25.attn_v.weight - [ 2304,  1024,     1,     1], type =    f16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n",
      "[ 288/ 288]                   output_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "llama_model_quantize_internal: model size  =  4986.92 MB\n",
      "llama_model_quantize_internal: quant size  =  1623.67 MB\n",
      "\n",
      "main: quantize time = 87391.38 ms\n",
      "main:    total time = 87391.38 ms\n"
     ]
    }
   ],
   "source": [
    "! llama.cpp/llama-quantize ./quantized_model_final/FP16.gguf ./quantized_model_final/ft_Q4_K_M.gguf Q4_K_M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b0f28d2-5f11-40ce-8292-0d3a8ddeba21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 3605 (cfac111e)\n",
      "main: built with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\n",
      "main: seed  = 1724250815\n",
      "llama_model_loader: loaded meta data with 33 key-value pairs and 288 tensors from ./quantized_model_final/sql_gpt_quantized.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Gemma 2 2b It\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Google\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = it\n",
      "llama_model_loader: - kv   5:                           general.basename str              = gemma-2\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 2B\n",
      "llama_model_loader: - kv   7:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   8:                    gemma2.embedding_length u32              = 2304\n",
      "llama_model_loader: - kv   9:                         gemma2.block_count u32              = 26\n",
      "llama_model_loader: - kv  10:                 gemma2.feed_forward_length u32              = 9216\n",
      "llama_model_loader: - kv  11:                gemma2.attention.head_count u32              = 8\n",
      "llama_model_loader: - kv  12:             gemma2.attention.head_count_kv u32              = 4\n",
      "llama_model_loader: - kv  13:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  14:                gemma2.attention.key_length u32              = 256\n",
      "llama_model_loader: - kv  15:              gemma2.attention.value_length u32              = 256\n",
      "llama_model_loader: - kv  16:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  17:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  18:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  19:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  23:                      tokenizer.ggml.scores arr[f32,256000]  = [-1000.000000, -1000.000000, -1000.00...\n",
      "llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  28:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  29:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  30:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  31:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  32:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:  105 tensors\n",
      "llama_model_loader: - type q4_K:  156 tensors\n",
      "llama_model_loader: - type q6_K:   27 tensors\n",
      "llm_load_vocab: special tokens cache size = 217\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma2\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 2304\n",
      "llm_load_print_meta: n_layer          = 26\n",
      "llm_load_print_meta: n_head           = 8\n",
      "llm_load_print_meta: n_head_kv        = 4\n",
      "llm_load_print_meta: n_rot            = 256\n",
      "llm_load_print_meta: n_swa            = 4096\n",
      "llm_load_print_meta: n_embd_head_k    = 256\n",
      "llm_load_print_meta: n_embd_head_v    = 256\n",
      "llm_load_print_meta: n_gqa            = 2\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 9216\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 2B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 2.61 B\n",
      "llm_load_print_meta: model size       = 1.59 GiB (5.21 BPW) \n",
      "llm_load_print_meta: general.name     = Gemma 2 2b It\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.13 MiB\n",
      "llm_load_tensors:        CPU buffer size =  1623.67 MiB\n",
      "..........................................................\n",
      "llama_new_context_with_model: n_ctx      = 8192\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   832.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  832.00 MiB, K (f16):  416.00 MiB, V (f16):  416.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   504.50 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1050\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "\n",
      "system_info: n_threads = 2 / 4 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature \n",
      "generate: n_ctx = 8192, n_batch = 2048, n_predict = 256, n_keep = 1\n",
      "\n",
      "\n",
      "### CONTEXT:\n",
      "CREATE TABLE table_name_58 (attendance VARCHAR, loss VARCHAR)\n",
      "\n",
      "### QUESTION:What was the attendance that had a loss of Ponson (1-5)?\n",
      "\n",
      "### [RESPONSE]:\n",
      "SELECT attendance FROM table_name_58 WHERE loss = \"ponson (1-5)\"\n",
      "\n",
      "### [RESPONSE]:\n",
      "10,818 Transfermarkt.de: 11.07.2008 - 21:00 (CET) | 2nd leg | champions league/european cup qualifiers | ponsen (1-5) | [RESPONSE]:\n",
      "10,818 Transfermarkt.de: 11.07.2008 - 21:00 (CET) | 2nd leg | champions league/european cup qualifiers | ponson (1-5) | [RESPONSE]:\n",
      "10,818 Transfermarkt.de: 11.07.2008 - 21:00 (CET) | 2nd leg | champions league/european cup qualifiers | ponson (1-5) | [RESPONSE]:\n",
      "10,818 Transfermarkt.de: 11.07.2008 - 21:00 (CET) | 2nd leg | champions league/european cup qualifiers | ponson (1-5) | [RESPONSE]:\n",
      "10,81\n",
      "llama_print_timings:        load time =    7190.32 ms\n",
      "llama_print_timings:      sample time =      42.59 ms /   256 runs   (    0.17 ms per token,  6011.08 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1696.42 ms /    46 tokens (   36.88 ms per token,    27.12 tokens per second)\n",
      "llama_print_timings:        eval time =   27435.88 ms /   255 runs   (  107.59 ms per token,     9.29 tokens per second)\n",
      "llama_print_timings:       total time =   29289.04 ms /   301 tokens\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "! ./llama.cpp/llama-cli -m ./quantized_model_final/sql_gpt_quantized.gguf -n 256 -p \"### CONTEXT:\\nCREATE TABLE table_name_58 (attendance VARCHAR, loss VARCHAR)\\n\\n### QUESTION:What was the attendance that had a loss of Ponson (1-5)?\\n\\n### [RESPONSE]:\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b0b9b290-fe70-428b-be4c-89ca1c93eb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi, HfFolder, create_repo, upload_file\n",
    "model_path = \"./quantized_model_final/sql_gpt_quantized.gguf\" # Your model's local path\n",
    "repo_name = \"sql-gpt-final-quantized-GGUF\"  # Desired HF Hub repository name\n",
    "repo_url = create_repo(repo_name, private=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c3941432-7358-499c-8114-f0f0f498a512",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a70a283fb2c3493895ec06f93c14005e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sql_gpt_quantized.gguf:   0%|          | 0.00/1.71G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/awais009/sql-gpt-final-quantized-GGUF/commit/64b7cc6e52968fe920c1cdd42b9eb030ca109965', commit_message='Upload sql_gpt_quantized.gguf with huggingface_hub', commit_description='', oid='64b7cc6e52968fe920c1cdd42b9eb030ca109965', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api = HfApi()\n",
    "api.upload_file(\n",
    "    path_or_fileobj=model_path,\n",
    "    path_in_repo=\"sql_gpt_quantized.gguf\",\n",
    "    repo_id=\"awais009/sql-gpt-final-quantized-GGUF\",\n",
    "    repo_type=\"model\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "185efedf-c99c-40b5-b955-b0b98534d394",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama-cpp-python in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.2.88)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from llama-cpp-python) (4.12.2)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from llama-cpp-python) (1.26.4)\n",
      "Requirement already satisfied: diskcache>=5.6.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from llama-cpp-python) (5.6.3)\n",
      "Requirement already satisfied: jinja2>=2.11.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from llama-cpp-python) (3.1.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jinja2>=2.11.3->llama-cpp-python) (2.1.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "164c4cec-158b-45b5-9a0e-5cfa0556cb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "638d007e-de40-4d1c-8ea5-658b1aa266fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 34 key-value pairs and 288 tensors from ./quantized_model/sql_gpt_quantized.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Gemma 2 2b It\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Google\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = it\n",
      "llama_model_loader: - kv   5:                           general.basename str              = gemma-2\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 2B\n",
      "llama_model_loader: - kv   7:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   8:                    gemma2.embedding_length u32              = 2304\n",
      "llama_model_loader: - kv   9:                         gemma2.block_count u32              = 26\n",
      "llama_model_loader: - kv  10:                 gemma2.feed_forward_length u32              = 9216\n",
      "llama_model_loader: - kv  11:                gemma2.attention.head_count u32              = 8\n",
      "llama_model_loader: - kv  12:             gemma2.attention.head_count_kv u32              = 4\n",
      "llama_model_loader: - kv  13:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  14:                gemma2.attention.key_length u32              = 256\n",
      "llama_model_loader: - kv  15:              gemma2.attention.value_length u32              = 256\n",
      "llama_model_loader: - kv  16:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  17:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  18:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  19:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  23:                      tokenizer.ggml.scores arr[f32,256000]  = [-1000.000000, -1000.000000, -1000.00...\n",
      "llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  28:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  29:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  30:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  31:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  32:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  33:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:  105 tensors\n",
      "llama_model_loader: - type q4_K:  156 tensors\n",
      "llama_model_loader: - type q6_K:   27 tensors\n",
      "llm_load_vocab: special tokens cache size = 249\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma2\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 2304\n",
      "llm_load_print_meta: n_layer          = 26\n",
      "llm_load_print_meta: n_head           = 8\n",
      "llm_load_print_meta: n_head_kv        = 4\n",
      "llm_load_print_meta: n_rot            = 256\n",
      "llm_load_print_meta: n_swa            = 4096\n",
      "llm_load_print_meta: n_embd_head_k    = 256\n",
      "llm_load_print_meta: n_embd_head_v    = 256\n",
      "llm_load_print_meta: n_gqa            = 2\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 9216\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 2B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 2.61 B\n",
      "llm_load_print_meta: model size       = 1.59 GiB (5.21 BPW) \n",
      "llm_load_print_meta: general.name     = Gemma 2 2b It\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.13 MiB\n",
      "llm_load_tensors:        CPU buffer size =  1623.67 MiB\n",
      "..........................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =    52.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   52.00 MiB, K (f16):   26.00 MiB, V (f16):   26.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   504.50 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1050\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.ggml.add_space_prefix': 'false', 'gemma2.attention.head_count': '8', 'gemma2.feed_forward_length': '9216', 'gemma2.block_count': '26', 'tokenizer.ggml.pre': 'default', 'general.type': 'model', 'gemma2.embedding_length': '2304', 'general.basename': 'gemma-2', 'tokenizer.ggml.padding_token_id': '0', 'gemma2.context_length': '8192', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'general.architecture': 'gemma2', 'gemma2.attention.key_length': '256', 'gemma2.attention.value_length': '256', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'general.finetune': 'it', 'general.file_type': '15', 'gemma2.attention.sliding_window': '4096', 'gemma2.attn_logit_softcapping': '50.000000', 'gemma2.final_logit_softcapping': '30.000000', 'general.organization': 'Google', 'gemma2.attention.head_count_kv': '4', 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.model': 'llama', 'general.name': 'Gemma 2 2b It', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '3', 'general.size_label': '2B', 'tokenizer.ggml.add_bos_token': 'true'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "Using chat eos_token: <eos>\n",
      "Using chat bos_token: <bos>\n"
     ]
    }
   ],
   "source": [
    "llm = Llama(\n",
    "  model_path=\"./quantized_model/sql_gpt_quantized.gguf\",  # Download the model file first\n",
    "  n_ctx=512,  # The max sequence length to use - note that longer sequence lengths require much more resources\n",
    "  n_threads=2,            # The number of CPU threads to use, tailor to your system and the resulting performance\n",
    "  n_gpu_layers=-1         # The number of layers to offload to GPU, if you have GPU acceleration available\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c643dad1-25b6-4bd7-922c-ea6c2e6e9499",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"CREATE TABLE table_name_82 (architect_name VARCHAR, project_name VARCHAR, completion_year VARCHAR)\"\n",
    "question = \"Which project by Frank Lloyd Wright was completed after 1935?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "628bdae7-983b-4984-9743-e240130728a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    2264.02 ms\n",
      "llama_print_timings:      sample time =      61.66 ms /   455 runs   (    0.14 ms per token,  7379.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2263.93 ms /    57 tokens (   39.72 ms per token,    25.18 tokens per second)\n",
      "llama_print_timings:        eval time =   47202.23 ms /   454 runs   (  103.97 ms per token,     9.62 tokens per second)\n",
      "llama_print_timings:       total time =   50188.97 ms /   511 tokens\n"
     ]
    }
   ],
   "source": [
    "query = f'''\n",
    "### CONTEXT:\\n{context}\\n\\n### QUESTION:{question}\\n\\n### [RESPONSE]:\\n\"\n",
    "'''\n",
    "output = llm(\n",
    "  prompt=query,\n",
    "  max_tokens=512,  # Generate up to 512 tokens\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "311895c0-0a6f-4bad-b6e9-b24b6ad61ece",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'cmpl-ff354890-1db7-442e-8555-6bccebe104b4',\n",
       " 'object': 'text_completion',\n",
       " 'created': 1724206712,\n",
       " 'model': './quantized_model_pro/sql_gpt_pro_quantized.gguf',\n",
       " 'choices': [{'text': 'SELECT architect_name FROM table_name_82 WHERE project_name = \"frank lloyd wright\" AND completion_year > 1935\\n\"\\n### [RESPONSE]:\\nSELECT architect_name FROM table_name_82 WHERE project_name = \"frank lloyd wright\" AND completion_year > 1935\\n### [RESPONSE]:\\nSELECT architect_name FROM table_name_82 WHERE project_name = \"frank lloyd wright\" AND completion_year > 1935\\n### [RESPONSE]:\\nSELECT architect_name FROM table_name_82 WHERE project_name = \"frank lloyd wright\" AND completion_year > 1935\\n### [RESPONSE]:\\nSELECT architect_name FROM table_name_82 WHERE project_name = \"frank lloyd wright\" AND completion_year > 1935\\n### [RESPONSE]:\\nSELECT architect_name FROM table_name_82 WHERE project_name = \"frank lloyd wright\" AND completion_year > 1935\\n### [RESPONSE]:\\nSELECT architect_name FROM table_name_82 WHERE project_name = \"frank lloyd wright\" AND completion_year > 1935\\n### [RESPONSE]:\\nSELECT architect_name FROM table_name_82 WHERE project_name = \"frank lloyd wright\" AND completion_year > 1935\\n### [RESPONSE]:\\nSELECT architect_name FROM table_name_82 WHERE project_name = \"frank lloyd wright\" AND completion_year > 1935\\n### [RESPONSE]:\\nSELECT architect_name FROM table_name_82 WHERE project_name = \"frank lloyd wright\" AND completion_year > 1935\\n### [RESPONSE]:\\nSELECT architect_name FROM table_name_82 WHERE project_name = \"frank lloyd wright\" AND completion_year > 1935\\n### [RESPONSE]:\\nSELECT architect_name FROM table_name_82 WHERE project_name = \"frank lloyd wright\" AND completion_year > 1935\\n### [',\n",
       "   'index': 0,\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'length'}],\n",
       " 'usage': {'prompt_tokens': 57, 'completion_tokens': 455, 'total_tokens': 512}}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2da51d55-9edd-4433-a1a0-0e801240c2a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT project_name FROM table_name_82 WHERE architect_name = 'Frank Lloyd Wright' AND completion_year > 1935 ORDER BY completion_year DESC LIMIT 1\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(output[\"choices\"][0][\"text\"].split('###')[0].strip('\"'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "273bc546-1839-4b2b-a65e-6abca75c222c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "803248a0-0fc0-4c09-97ac-19f23f09101c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8880df5a472e4fccb3cdde3305082a17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/4.43k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53ef01538f2840f090c11ee9a4a8bccd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/21.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0061931072f948e087f7eca359b51a5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/78577 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"b-mc2/sql-create-context\", split=\"train[68000:69000]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a5351547-e052-4def-a9e1-47b7848d226f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataset.to_pandas().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b32e420c-cdf0-420f-9e31-9313277e2632",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e5cb646c-f887-4a6f-969e-644cdc396f31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SELECT party FROM table_name_91 WHERE district = \"south carolina 2\"'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[56, :]['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "aa0e1943-4662-4025-9f67-b22da605ef2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CREATE TABLE table_name_91 (party VARCHAR, district VARCHAR)'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[56, :]['context']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "29bf2a6c-f539-470d-b134-0fa00c58f07d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What party has the South Carolina 2 district?'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[56, :]['question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9add1102-d943-48be-98c7-715c21e4d69f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
